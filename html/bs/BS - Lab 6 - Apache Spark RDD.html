<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        BS - Lab 6 - Apache Spark RDD - CodiMD
    </title>
    <link rel="icon" type="image/png" href="http://localhost:3000/favicon.png">
    <link rel="apple-touch-icon" href="http://localhost:3000/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.0/css/bootstrap.min.css" integrity="sha256-H0KfTigpUV+0/5tn2HXC0CPwhhDhWgSawJdnFd0CGCo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fork-awesome/1.1.3/css/fork-awesome.min.css" integrity="sha256-ZhApazu+kejqTYhMF+1DzNKjIzP7KXu6AzyXcC1gMus=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github-gist.min.css" integrity="sha256-tAflq+ymku3Khs+I/WcAneIlafYgDiOQ9stIHH985Wo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@hackmd/emojify.js@2.1.0/dist/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,600,600italic,300italic,300|Source+Serif+Pro|Source+Code+Pro:400,300,500&subset=latin,latin-ext);.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol;font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{padding:0 1em;color:#777;border-left:.25em solid #ddd}.night .markdown-body blockquote{color:#bcbcbc}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.night .markdown-body h1,.night .markdown-body h2,.night .markdown-body h3,.night .markdown-body h4,.night .markdown-body h5,.night .markdown-body h6{color:#ddd}.markdown-body h1 .fa-link,.markdown-body h2 .fa-link,.markdown-body h3 .fa-link,.markdown-body h4 .fa-link,.markdown-body h5 .fa-link,.markdown-body h6 .fa-link{color:#000;vertical-align:middle;visibility:hidden;font-size:16px}.night .markdown-body h1 .fa-link,.night .markdown-body h2 .fa-link,.night .markdown-body h3 .fa-link,.night .markdown-body h4 .fa-link,.night .markdown-body h5 .fa-link,.night .markdown-body h6 .fa-link{color:#fff}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .fa-link,.markdown-body h2:hover .anchor .fa-link,.markdown-body h3:hover .anchor .fa-link,.markdown-body h4:hover .anchor .fa-link,.markdown-body h5:hover .anchor .fa-link,.markdown-body h6:hover .anchor .fa-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.night .markdown-body table tr{background-color:#5f5f5f}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.night .markdown-body table tr:nth-child(2n){background-color:#4f4f4f}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.night .markdown-body code,.night .markdown-body tt{color:#eee;background-color:hsla(0,0%,90.2%,.36)}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\A0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.markdown-body kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid;border-color:#ccc #ccc #bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol;padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important}.markdown-body pre{border:inherit!important}.night .markdown-body pre{filter:invert(100%)}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-webkit-inline-flex;display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.night .markdown-body .gist table tr:nth-child(2n){background-color:#ddd}.markdown-body code[data-gist-id]{background:none;padding:0;filter:invert(100%)}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.geo,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram,.markdown-body pre.vega{text-align:center;background-color:inherit;border-radius:0;white-space:inherit}.night .markdown-body pre.graphviz .graph>polygon{fill:#333}.night .markdown-body pre.mermaid .sectionTitle,.night .markdown-body pre.mermaid .titleText,.night .markdown-body pre.mermaid text{fill:#fff}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code,.markdown-body pre.vega>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg,.markdown-body pre.vega>svg{max-width:100%;height:100%}.night .markdown-body .abc path{fill:#eee}.night .markdown-body .abc path.note_selected{fill:##4DD0E1}.night tspan{fill:#fefefe}.night pre rect{fill:transparent}.night pre.flow-chart path,.night pre.flow-chart rect{stroke:#fff}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p,.markdown-body .alert>ul{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body img{background-color:transparent}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;-webkit-transition:opacity .2s;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;-webkit-transition:opacity .2s;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.geo-map{width:100%;height:250px}.markmap-container{height:300px}.markmap-container>svg{width:100%;height:100%}.MJX_Assistive_MathML{display:none}.ui-infobar{position:relative;z-index:2;max-width:758px;margin-top:25px;margin-bottom:-25px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:10000}.ui-toc-label{opacity:.9;background-color:#ccc;border:none}.ui-toc-label,.ui-toc .open .ui-toc-label{-webkit-transition:opacity .2s;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#5f5f5f}.ui-toc-label:focus{opacity:1;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;-webkit-transition:opacity .2s;transition:opacity .2s}.ui-toc-dropdown{margin-top:23px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child>ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.night .ui-toc-dropdown .nav>li>a:focus,.night .ui-toc-dropdown .nav>li>a:hover{color:#fff;border-left-color:#fff}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.night .ui-toc-dropdown .nav>.active:focus>a,.night .ui-toc-dropdown .nav>.active:hover>a,.night .ui-toc-dropdown .nav>.active>a{color:#fff;border-left:2px solid #fff}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.night .ui-toc-dropdown .nav>li>a{color:#aaa}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:50px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>a{padding-right:50px}.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:60px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>ul>li>a{padding-right:60px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>a:hover{padding-left:49px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>a:hover{padding-right:49px}.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>ul>li>a:hover{padding-left:59px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>ul>li>a:hover{padding-right:59px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active>a{padding-left:48px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.active>.nav>.nav>.active>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>.nav>.active:hover>a{padding-right:48px}.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active>a{padding-left:58px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.active>.nav>.nav>.active>.nav>.active>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active:hover>a{padding-right:58px}.markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Helvetica,Arial,Hiragino Kaku Gothic Pro,"\30D2\30E9\30AE\30CE\89D2\30B4   Pro W3",Osaka,Meiryo,"\30E1\30A4\30EA\30AA",MS Gothic,"\FF2D\FF33   \30B4\30B7\30C3\30AF",sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}.ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,"\FF2D\FF33   \FF30\30B4\30B7\30C3\30AF",sans-serif}.markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Helvetica,Arial,PingFang TC,Microsoft JhengHei,"\5FAE\8EDF\6B63\9ED1",sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}.ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,"\5FAE\8EDF\6B63\9ED1UI",sans-serif}.markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Helvetica,Arial,PingFang SC,Microsoft YaHei,"\5FAE\8F6F\96C5\9ED1",sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}.ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,"\5FAE\8F6F\96C5\9ED1UI",sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:rgba(0,0,0,.85)}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:3px;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:contain}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}small span{line-height:22px}small .dropdown{display:inline-block}small .dropdown a:focus,small .dropdown a:hover{text-decoration:none}.unselectable{-moz-user-select:none;-khtml-user-select:none;-webkit-user-select:none;-o-user-select:none;user-select:none}.night .navbar{background:#333;border-bottom-color:#333;color:#eee}.night .navbar a{color:#eee}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;font-family:Source Sans Pro,Helvetica,Arial,sans-serif;letter-spacing:.025em}.focus,:focus{outline:none!important}::-moz-focus-inner{border:0!important}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid" lang="en"><h1 id="Lab-6---Apache-Spark-RDD"><a class="anchor hidden-xs" href="#Lab-6---Apache-Spark-RDD" title="Lab-6---Apache-Spark-RDD"><i class="fa fa-link"></i></a>Lab 6 - Apache Spark RDD</h1><p><strong>Course:</strong> Big Data - IU S25<br>
<strong>Author:</strong> Firas Jolha</p><h2 id="Datasets"><a class="anchor hidden-xs" href="#Datasets" title="Datasets"><i class="fa fa-link"></i></a>Datasets</h2><ul>
<li><a href="https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt" target="_blank" rel="noopener">The Adventures of Sherlock Holmes E. 12</a></li>
<li><a href="https://github.com/firas-jolha/fjiubd2024/raw/main/data/movies.csv" target="_blank" rel="noopener">Top gross movies between 2007 and 2011</a></li>
<li><a href="https://raw.githubusercontent.com/aminebennaji19/FIFA-World-Cup-Qatar-2022/main/data/results.csv" target="_blank" rel="noopener">World cup results from 1872 till 2022</a></li>
</ul><h2 id="PySpark-on-Colab"><a class="anchor hidden-xs" href="#PySpark-on-Colab" title="PySpark-on-Colab"><i class="fa fa-link"></i></a>PySpark on Colab</h2><ul>
<li><a href="https://colab.research.google.com/drive/1HYtvI_a4ZIhq6nmMwKa3wPoaxniEW2vv?usp=sharing" target="_blank" rel="noopener">PySparkOnColab.ipynb</a></li>
</ul><h2 id="Readings"><a class="anchor hidden-xs" href="#Readings" title="Readings"><i class="fa fa-link"></i></a>Readings</h2><ul>
<li><a href="https://spark.apache.org/docs/latest/api/python/index.html" target="_blank" rel="noopener">Spark 3 Python API Docs</a></li>
</ul><h1 id="Agenda"><a class="anchor hidden-xs" href="#Agenda" title="Agenda"><i class="fa fa-link"></i></a>Agenda</h1><p></p><div class="toc"><ul>
<li><a href="#Lab-6---Apache-Spark-RDD" title="Lab 6 - Apache Spark RDD">Lab 6 - Apache Spark RDD</a><ul>
<li><a href="#Datasets" title="Datasets">Datasets</a></li>
<li><a href="#PySpark-on-Colab" title="PySpark on Colab">PySpark on Colab</a></li>
<li><a href="#Readings" title="Readings">Readings</a></li>
</ul>
</li>
<li><a href="#Agenda" title="Agenda">Agenda</a></li>
<li><a href="#Prerequisites" title="Prerequisites">Prerequisites</a></li>
<li><a href="#Objectives" title="Objectives">Objectives</a></li>
<li><a href="#Intro-to-Apache-Spark-review" title="Intro to Apache Spark [review]">Intro to Apache Spark [review]</a><ul>
<li><a href="#Core-Concepts-in-Spark" title="Core Concepts in Spark">Core Concepts in Spark</a></li>
<li><a href="#Spark-Application-Model" title="Spark Application Model">Spark Application Model</a></li>
<li><a href="#Spark-Execution-Model" title="Spark Execution Model">Spark Execution Model</a></li>
<li><a href="#Spark-Components" title="Spark Components">Spark Components</a></li>
<li><a href="#Spark-Architecture" title="Spark Architecture">Spark Architecture</a></li>
<li><a href="#How-Spark-works" title="How Spark works?">How Spark works?</a></li>
<li><a href="#Spark-Features" title="Spark Features">Spark Features</a></li>
<li><a href="#Supported-Cluster-Managers" title="Supported Cluster Managers">Supported Cluster Managers</a></li>
</ul>
</li>
<li><a href="#PySpark" title="PySpark">PySpark</a><ul>
<li><a href="#PySpark-Modules-amp-Packages" title="PySpark Modules &amp; Packages">PySpark Modules &amp; Packages</a></li>
<li><a href="#Install-PySpark" title="Install PySpark">Install PySpark</a><ul>
<li><a href="#On-Colab" title="On Colab">On Colab</a></li>
<li><a href="#Using-pip" title="Using pip">Using pip</a></li>
<li><a href="#Using-Docker" title="Using Docker">Using Docker</a></li>
<li><a href="#Deploying-Spark-on-Yarn-Cluster" title="Deploying Spark on Yarn Cluster">Deploying Spark on Yarn Cluster</a></li>
<li><a href="#Optional-Install-PySpark-on-HDP-Sandbox" title="[Optional] Install PySpark on HDP Sandbox">[Optional] Install PySpark on HDP Sandbox</a></li>
</ul>
</li>
<li><a href="#Running-Spark-applications" title="Running Spark applications">Running Spark applications</a><ul>
<li><a href="#pyspark-shell" title="pyspark shell">pyspark shell</a></li>
<li><a href="#spark-submit" title="spark-submit">spark-submit</a></li>
</ul>
</li>
<li><a href="#Python-Package-Management-in-PySpark-apps" title="Python Package Management in PySpark apps">Python Package Management in PySpark apps</a></li>
</ul>
</li>
<li><a href="#Spark-Web-UI" title="Spark Web UI">Spark Web UI</a><ul>
<li><a href="#Jobs-Tab" title="Jobs Tab">Jobs Tab</a><ul>
<li><a href="#Job-details" title="Job details">Job details</a></li>
</ul>
</li>
<li><a href="#Stages-Tab" title="Stages Tab">Stages Tab</a><ul>
<li><a href="#Stage-details" title="Stage details">Stage details</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#Spark-Core" title="Spark Core">Spark Core</a><ul>
<li><a href="#Spark-RDD-Operations" title="Spark RDD Operations">Spark RDD Operations</a></li>
<li><a href="#Spark-RDD-Transformations" title="Spark RDD Transformations">Spark RDD Transformations</a></li>
<li><a href="#Spark-RDD-Actions" title="Spark RDD Actions">Spark RDD Actions</a></li>
</ul>
</li>
<li><a href="#Spark-Context" title="Spark Context">Spark Context</a><ul>
<li><a href="#Import-required-packages" title="Import required packages">Import required packages</a></li>
<li><a href="#Create-a-SparkContext" title="Create a SparkContext">Create a SparkContext</a></li>
</ul>
</li>
<li><a href="#Spark-RDD" title="Spark RDD">Spark RDD</a><ul>
<li class="invisable-node"><ul>
<li><a href="#1-using-parallelize-function" title="1. using parallelize() function">1. using parallelize() function</a></li>
<li><a href="#2-using-textFile-or-wholeTextFiles-functions" title="2. using textFile or wholeTextFiles functions">2. using textFile or wholeTextFiles functions</a></li>
</ul>
</li>
<li><a href="#Create-empty-RDD" title="Create empty RDD">Create empty RDD</a></li>
<li><a href="#Repartition-and-Coalesce" title="Repartition and Coalesce">Repartition and Coalesce</a></li>
<li><a href="#PySpark-RDD-Transformations" title="PySpark RDD Transformations">PySpark RDD Transformations</a><ul>
<li><a href="#map-and-flatMap" title="map and flatMap">map and flatMap</a></li>
<li><a href="#filter" title="filter">filter</a></li>
<li><a href="#distinct" title="distinct">distinct</a></li>
<li><a href="#sample" title="sample">sample</a></li>
<li><a href="#randomSplit" title="randomSplit">randomSplit</a></li>
<li><a href="#mapPartitions-and-mapPartitionsWithIndex" title="mapPartitions and mapPartitionsWithIndex">mapPartitions and mapPartitionsWithIndex</a></li>
<li><a href="#sortBy-and-groupBy" title="sortBy and groupBy">sortBy and groupBy</a></li>
<li><a href="#sortByKey-and-reduceByKey" title="sortByKey and reduceByKey">sortByKey and reduceByKey</a></li>
</ul>
</li>
<li><a href="#PySpark-RDD-Actions" title="PySpark RDD Actions">PySpark RDD Actions</a><ul>
<li><a href="#collect" title="collect">collect</a></li>
<li><a href="#max-min-first-top-take" title="max, min, first, top, take">max, min, first, top, take</a></li>
<li><a href="#count-countByValue" title="count, countByValue">count, countByValue</a></li>
<li><a href="#reduce-treeReduce" title="reduce, treeReduce">reduce, treeReduce</a></li>
<li><a href="#saveAsTextFile" title="saveAsTextFile">saveAsTextFile</a></li>
</ul>
</li>
<li><a href="#RDD-Persistence" title="RDD Persistence">RDD Persistence</a><ul>
<li><a href="#RDD-Cache" title="RDD Cache">RDD Cache</a></li>
<li><a href="#RDD-Persist" title="RDD Persist">RDD Persist</a></li>
<li><a href="#RDD-Unpersist" title="RDD Unpersist">RDD Unpersist</a></li>
</ul>
</li>
<li><a href="#Shuffling-in-Spark-engine" title="Shuffling in Spark engine">Shuffling in Spark engine</a></li>
<li><a href="#Shared-Variables" title="Shared Variables">Shared Variables</a><ul>
<li><a href="#Broadcast-variables" title="Broadcast variables">Broadcast variables</a></li>
<li><a href="#Accumulator-variables" title="Accumulator variables">Accumulator variables</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#References" title="References">References</a></li>
</ul>
</div><p></p><h1 id="Prerequisites"><a class="anchor hidden-xs" href="#Prerequisites" title="Prerequisites"><i class="fa fa-link"></i></a>Prerequisites</h1><ul>
<li>You have a running Hadoop Cluster with Spark integration</li>
</ul><h1 id="Objectives"><a class="anchor hidden-xs" href="#Objectives" title="Objectives"><i class="fa fa-link"></i></a>Objectives</h1><ul>
<li>Transformations &amp; actions in pyspark</li>
<li>Learn how to analyze data in pyspark</li>
<li>Write spark applications</li>
</ul><h1 id="Intro-to-Apache-Spark-review"><a class="anchor hidden-xs" href="#Intro-to-Apache-Spark-review" title="Intro-to-Apache-Spark-review"><i class="fa fa-link"></i></a>Intro to Apache Spark [review]</h1><div class="alert alert-warning">
<p>This section gives a theoretical introduction to Spark, which will be covered in the lecture. Feel free to skip it if you have enough theoretical knowledge about Spark.</p>
</div><p><strong>Apache Spark</strong> is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads (only for spark 3), <strong>MLlib</strong> for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.</p><p><img src="https://i.imgur.com/N20Uo5D.png" alt="" class="md-image md-image"></p><p>Spark introduces the concept of an <strong>RDD (Resilient Distributed Dataset)</strong>, an <strong>immutable</strong> fault-tolerant, distributed collection of objects that can be operated on in parallel. An RDD can contain any type of object and is created by loading an external dataset or distributing a collection from the driver program. Each RDD is split into multiple partitions (similar pattern with smaller sets), which may be computed on different nodes of the cluster</p><p>RDD means:<br>
<strong>Resilient</strong> – capable of rebuilding data on failure<br>
<strong>Distributed</strong> – distributes data among various nodes in cluster<br>
<strong>Dataset</strong> – collection of partitioned data with values</p><h2 id="Core-Concepts-in-Spark"><a class="anchor hidden-xs" href="#Core-Concepts-in-Spark" title="Core-Concepts-in-Spark"><i class="fa fa-link"></i></a>Core Concepts in Spark</h2><ul>
<li><strong>Application:</strong> A user program built on Spark. Consists of a driver program and executors on the cluster.</li>
<li><strong>Driver Program:</strong> The process running the <code>main()</code> function of the application and creating the <code>SparkContext</code>.</li>
<li><strong>Cluster Manager:</strong> An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN). You can set it using <code>--master</code> option in <code>spark-submit</code> tool.</li>
<li><strong>Deploy mode:</strong> Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster. <code>--deploy-mode</code> option in <code>spark-submit</code> tool.</li>
<li><strong>Job:</strong> A piece of code which reads some input from HDFS or local, performs some computation on the data and writes some output data.</li>
<li><strong>Stages:</strong> Jobs are divided into stages. Stages are classified as a Map or reduce stages. Stages are divided based on computational boundaries, all computations (operators) cannot be updated in a single Stage. It happens over many stages.</li>
<li><strong>Tasks:</strong> Each stage has some tasks, one task per partition. One task is executed on one partition of data on one executor (machine).</li>
<li><strong>DAG:</strong> DAG stands for Directed Acyclic Graph, in the present context its a DAG of operators.</li>
<li><strong>Executor:</strong> The process responsible for executing a task.</li>
<li><strong>Master:</strong> The machine on which the Driver program runs</li>
<li><strong>Slave/Worker:</strong> The machine on which the Executor program runs</li>
</ul><p>Check the glossary from <a href="https://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">here</a>.</p><h2 id="Spark-Application-Model"><a class="anchor hidden-xs" href="#Spark-Application-Model" title="Spark-Application-Model"><i class="fa fa-link"></i></a>Spark Application Model</h2><p>Apache Spark is widely considered to be the <strong>successor</strong> to MapReduce for general purpose data processing on Apache Hadoop clusters. In <strong>MapReduce</strong>, the highest-level unit of computation is a <strong>job</strong>. A job loads data, applies a map function, shuffles it, applies a reduce function, and writes data back out to persistent storage. In <strong>Spark</strong>, the highest-level unit of computation is an <strong>application</strong>. A Spark application can be used for a single batch job, an interactive session with multiple jobs, or a long-lived server continually satisfying requests. <strong>A Spark job can consist of more than just a single map and reduce.</strong></p><p>MapReduce starts a process for each task. In contrast, a Spark application can have processes running on its behalf even when it is not running a job. Furthermore, multiple tasks can run within the same executor. Both combine to enable extremely fast task startup time as well as in-memory data storage, resulting in orders of magnitude faster performance over MapReduce.</p><h2 id="Spark-Execution-Model"><a class="anchor hidden-xs" href="#Spark-Execution-Model" title="Spark-Execution-Model"><i class="fa fa-link"></i></a>Spark Execution Model</h2><p>At runtime, a Spark application maps to a single driver process and a set of executor processes distributed across the hosts in a cluster.</p><p>The <strong>driver process</strong> manages the job flow and schedules tasks and is available the entire time the application is running. Typically, this driver process is the same as the client process used to initiate the job, although when run on YARN, the driver can run in the cluster. <strong>In interactive mode, the shell itself is the driver process.</strong></p><p>The executors are responsible for executing work, in the form of tasks, as well as for storing any data that you cache. Executor lifetime depends on whether dynamic allocation is enabled. An executor has a number of slots for running tasks, and will run many concurrently throughout its lifetime.</p><p><img src="https://i.imgur.com/kErC1WW.png" alt="" class="md-image md-image"></p><p>Invoking an <strong><code>action</code> operation</strong> inside a Spark application triggers the launch of a <strong>job</strong> to fulfill it. Spark examines the dataset on which that action depends and formulates an execution plan. The execution plan assembles the dataset transformations into <strong>stages</strong>. A stage is a collection of <strong>tasks</strong> that run the same code, each on a different subset of the data.</p><h2 id="Spark-Components"><a class="anchor hidden-xs" href="#Spark-Components" title="Spark-Components"><i class="fa fa-link"></i></a>Spark Components</h2><ul>
<li>Spark Driver</li>
<li>Executors</li>
<li>Cluster Manager</li>
</ul><p><img src="https://i.imgur.com/K7xzq0s.png" alt="" class="md-image md-image"></p><p>Spark Driver contains more components responsible for translation of user code into actual jobs executed on cluster:</p><p><img src="https://i.imgur.com/OMrw8jE.png" alt="" class="md-image md-image"></p><ul>
<li><strong>SparkContext</strong>
<ul>
<li>represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster</li>
</ul>
</li>
<li><strong>DAGScheduler</strong>
<ul>
<li>computes a DAG of stages for each job and submits them to TaskScheduler determines preferred locations for tasks (based on cache status or shuffle files locations) and finds minimum schedule to run the jobs</li>
</ul>
</li>
<li><strong>TaskScheduler</strong>
<ul>
<li>responsible for sending tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers</li>
</ul>
</li>
<li><strong>SchedulerBackend</strong>
<ul>
<li>backend interface for scheduling systems that allows plugging in different implementations( Mesos, YARN, Standalone, local)</li>
</ul>
</li>
<li><strong>BlockManager</strong>
<ul>
<li>provides interfaces for putting and retrieving blocks both locally and remotely into various stores (memory, disk, and off-heap)</li>
</ul>
</li>
</ul><h2 id="Spark-Architecture"><a class="anchor hidden-xs" href="#Spark-Architecture" title="Spark-Architecture"><i class="fa fa-link"></i></a>Spark Architecture</h2><p>Apache Spark works in a master-slave architecture where the <strong>master</strong> is called <strong>“Driver”</strong> and <strong>slaves</strong> are called <strong>“Workers”</strong>.</p><p><img src="https://i.imgur.com/3Tj1QUG.png" alt="" class="md-image md-image"></p><p>When you run a Spark application, Spark Driver creates a context that is an entry point to your application, and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by Cluster Manager.</p><h2 id="How-Spark-works"><a class="anchor hidden-xs" href="#How-Spark-works" title="How-Spark-works"><i class="fa fa-link"></i></a>How Spark works?</h2><p>Spark has a small code base and the system is divided in various layers. Each layer has some responsibilities. The layers are independent of each other.</p><p><img src="https://i.imgur.com/vLg1PoN.png" alt="" class="md-image md-image"></p><p>The first layer is the interpreter, Spark uses a Scala interpreter, with some modifications. As you enter your code in spark console (creating RDD’s and applying operators), Spark creates a operator graph. When the user runs an action (like collect), the Graph is submitted to a DAG Scheduler. The DAG scheduler divides operator graph into (map and reduce) stages. A stage is comprised of tasks based on partitions of the input data. The DAG scheduler pipelines operators together to optimize the graph. For e.g. Many map operators can be scheduled in a single stage. This optimization is key to Spark’s performance. The final result of a DAG scheduler is a set of stages. The stages are passed on to the Task Scheduler. The task scheduler launches tasks via cluster manager (Spark Standalone/Yarn/Mesos). The task scheduler doesn’t know about dependencies among stages.</p><h2 id="Spark-Features"><a class="anchor hidden-xs" href="#Spark-Features" title="Spark-Features"><i class="fa fa-link"></i></a>Spark Features</h2><ul>
<li>In-memory computation
<ul>
<li>PySpark loads the data from disk and process in memory and keeps the data in memory.</li>
<li>This is the main difference between PySpark and Mapreduce (I/O intensive).</li>
</ul>
</li>
<li>Distributed processing using parallelize
<ul>
<li>When you create RDD from a data, it partitions the data elements. By default, Spark creates one partition for each core.</li>
</ul>
</li>
<li>Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)</li>
<li>Fault-tolerant
<ul>
<li>It can be used to read and write files from distributed file systems like HDFS.</li>
</ul>
</li>
<li>Immutable
<ul>
<li>Once RDDs are created they cannot be modified, they need to be destroyed and recreated to perform any change.</li>
</ul>
</li>
<li>Lazy evaluation
<ul>
<li>PySpark does not evaluate the RDD transformations as they appear/encountered by Driver, instead it keeps the all transformations as it encounters in a graph (DAG) and evaluates them when it sees the first RDD action.</li>
</ul>
</li>
<li>Cache &amp; persistence
<ul>
<li>We can also cache/persists the RDD in memory to reuse the previous computations.</li>
</ul>
</li>
<li>Inbuild-optimization when using DataFrames</li>
<li>Supports SQL
<ul>
<li>We can perform analysis on the cluster via queries written in SQL and executed on Spark engine.</li>
</ul>
</li>
</ul><h2 id="Supported-Cluster-Managers"><a class="anchor hidden-xs" href="#Supported-Cluster-Managers" title="Supported-Cluster-Managers"><i class="fa fa-link"></i></a>Supported Cluster Managers</h2><p>Spark supports four cluster managers:</p><ul>
<li><em>Standalone</em> – a simple cluster manager included with Spark that makes it easy to set up a cluster.</li>
<li><em>Hadoop YARN</em> – the resource manager in Hadoop 2. This is mostly used, cluster manager.</li>
<li><em>Apache Mesos [deprecated]</em> – Mesos is a Cluster manager that can also run Hadoop MapReduce and PySpark applications.</li>
<li><em>Kubernetes</em> – an open-source system for automating deployment, scaling, and management of containerized applications.</li>
<li><strong>local</strong> – which is not really a cluster manager but still you can use “local” for master() in order to run Spark on your local machine.</li>
</ul><div class="alert alert-warning">
<p><a href="https://www.oreilly.com/library/view/data-algorithms-with/9781492082378/ch01.html" target="_blank" rel="noopener">Scala is the native language</a> for writing Spark applications but Apache Spark supports drivers for other languages such as Python (PySpark package), Java, and R.</p>
</div><h1 id="PySpark"><a class="anchor hidden-xs" href="#PySpark" title="PySpark"><i class="fa fa-link"></i></a>PySpark</h1><p>PySpark is a Spark library written in Python to run Python applications using Apache Spark capabilities. Using PySpark we can run applications in parallel on the distributed cluster (multiple nodes). In other words, PySpark is a Python API for Apache Spark.</p><p><strong>Spark</strong> is written in <strong>Scala</strong> and later on due to its industry adaptation, its API PySpark released for Python using Py4J Java library that is integrated within PySpark and allows Python to dynamically interface with JVM objects. Hence, to run PySpark you need Java to be installed along with Python, and Apache Spark.</p><h2 id="PySpark-Modules-amp-Packages"><a class="anchor hidden-xs" href="#PySpark-Modules-amp-Packages" title="PySpark-Modules-amp-Packages"><i class="fa fa-link"></i></a>PySpark Modules &amp; Packages</h2><ul>
<li><strong>PySpark RDD</strong> (<code>pyspark.rdd</code>)</li>
<li><strong>PySpark DataFrame and SQL</strong> (<code>pyspark.sql</code>)
<ul>
<li>Pandas-On-Spark (new in PySpark 3.2.0+)
<ul>
<li><code>pyspark.pandas</code></li>
</ul>
</li>
</ul>
</li>
<li>PySpark ML
<ul>
<li>RDD-based (<code>pyspark.mllib</code>)</li>
<li>Spark DataFrame-based (<code>pyspark.ml</code>)</li>
<li>Next week.</li>
</ul>
</li>
<li>PySpark Streaming (<code>pyspark.streaming</code>)
<ul>
<li>Structured Streaming (new in PySpark 3.2.0+)
<ul>
<li><code>pyspark.sql.streaming</code></li>
</ul>
</li>
<li>Last week.</li>
</ul>
</li>
<li>PySpark GraphFrames (GraphFrames)
<ul>
<li>Spark GraphX supported only in Scala</li>
<li>Last week.</li>
</ul>
</li>
<li>PySpark Resource (<code>pyspark.resource</code>)
<ul>
<li>new in PySpark 3.0</li>
</ul>
</li>
</ul><p>Besides these, if you wanted to use third-party libraries, you can find them at <a href="https://spark-packages.org/" target="_blank" rel="noopener">https://spark-packages.org/</a> . This page is kind of a repository of all Spark third-party libraries.</p><h2 id="Install-PySpark"><a class="anchor hidden-xs" href="#Install-PySpark" title="Install-PySpark"><i class="fa fa-link"></i></a>Install PySpark</h2><p>There are different ways to install Apache Spark on your machine.</p><h3 id="On-Colab"><a class="anchor hidden-xs" href="#On-Colab" title="On-Colab"><i class="fa fa-link"></i></a>On Colab</h3><p>You can use the notebook shared in the beginning of this tutorial for installation approach to run PySpark on Colab Notebook.</p><ul>
<li><a href="https://colab.research.google.com/drive/1HYtvI_a4ZIhq6nmMwKa3wPoaxniEW2vv?usp=sharing" target="_blank" rel="noopener">PySparkOnColab.ipynb</a></li>
</ul><h3 id="Using-pip"><a class="anchor hidden-xs" href="#Using-pip" title="Using-pip"><i class="fa fa-link"></i></a>Using pip</h3><p>You can install <code>pyspark</code> for Python language by simply installing the package <code>pyspark</code> using <code>pip</code> as follows:</p><ol>
<li>Create a virtual environment before installing the package.</li>
</ol><pre><code class="yaml hljs"><span class="hljs-string">python3</span> <span class="hljs-bullet">-m</span> <span class="hljs-string">venv</span> <span class="hljs-string">venv</span>
<span class="hljs-string">source</span> <span class="hljs-string">venv/bin/activate</span>
</code></pre><ol start="2">
<li>Install the package</li>
</ol><pre><code class="yaml hljs"><span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">pyspark</span>
</code></pre><p>This will install two packages <code>pyspark</code> and <code>py4j</code>. <code>py4j</code> is Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. This will install <code>Spark</code> software and you will be able to run Spark applications.</p><h3 id="Using-Docker"><a class="anchor hidden-xs" href="#Using-Docker" title="Using-Docker"><i class="fa fa-link"></i></a>Using Docker</h3><p>You can also install it using Docker as below.</p><ol>
<li>Pull the image <code>spark:python3</code></li>
</ol><pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">pull</span> <span class="hljs-attr">spark:python3</span>
</code></pre><ol start="2">
<li>Run a container and access the <code>pyspark</code> shell.</li>
</ol><pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">run</span> <span class="hljs-bullet">--name</span> <span class="hljs-string">pyspark</span> <span class="hljs-bullet">-it</span> <span class="hljs-bullet">-p</span> <span class="hljs-number">4040</span><span class="hljs-string">:4040</span> <span class="hljs-attr">spark:python3</span> <span class="hljs-string">/opt/spark/bin/pyspark</span>
</code></pre><p>Here we create a container from the image above, publish the port 4040 for the Spark Web UI and run the <code>pyspark</code> shell.</p><h3 id="Deploying-Spark-on-Yarn-Cluster"><a class="anchor hidden-xs" href="#Deploying-Spark-on-Yarn-Cluster" title="Deploying-Spark-on-Yarn-Cluster"><i class="fa fa-link"></i></a>Deploying Spark on Yarn Cluster</h3><p>You can deploy a fully-distributed Hadoop cluster where YARN is the resource manager. I have prepared a repository which can deploy the cluster using Docker containers. This cluster also can be used for running your Spark applications on the cluster. You can check the repository from <a href="https://github.com/firas-jolha/docker-spark-yarn-cluster" target="_blank" rel="noopener">here</a>. In order to start the cluster, you need to follow the steps below:</p><ol>
<li>Clone the repository (<a href="https://github.com/firas-jolha/docker-spark-yarn-cluster" target="_blank" rel="noopener">https://github.com/firas-jolha/docker-spark-yarn-cluster</a>) in a new folder</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-string">mkdir</span> <span class="hljs-bullet">-p</span> <span class="hljs-string">~/spark-yarn-test</span> <span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">cd</span> <span class="hljs-string">~/spark-yarn-test</span>
<span class="hljs-string">git</span> <span class="hljs-string">clone</span> <span class="hljs-string">git@github.com:firas-jolha/docker-spark-yarn-cluster.git</span>
<span class="hljs-string">cd</span> <span class="hljs-string">docker-spark-yarn-cluster</span>
</code></pre><ol start="2">
<li>Run the script <code>startHadoopCluster.sh</code> passing the number of slaves <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 1.08em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.919em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1000.92em, 2.481em, -999.997em); top: -2.314em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.11em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.32em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-1">N</script></span> (default value is 2).</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-string">N=5</span>
<span class="hljs-string">bash</span> <span class="hljs-string">startHadoopCluster.sh</span> <span class="hljs-string">$N</span>
</code></pre><p>Here I am running a cluster of one master node and 5 slave nodes. In HDFS, the master node will run the namenode and seconadry namenodes and the slave nodes will run the datanodes. In YARN, the master node will run the resource manager and the slave nodes will run the node managers. In Spark, the master node will run the Spark Cluster master and the slave nodes will run the workers.</p><ol start="3">
<li>Access the master node.</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">exec</span> <span class="hljs-bullet">-it</span> <span class="hljs-string">cluster-master</span> <span class="hljs-string">bash</span>
</code></pre><ol start="4">
<li>Check if the services are running. If some are not running then restart them.</li>
</ol><pre><code class="wrap yaml hljs">
<span class="hljs-string">jps</span> <span class="hljs-bullet">-lm</span>



<span class="hljs-comment"># Starting HDFS</span>
<span class="hljs-string">start-dfs.sh</span>

<span class="hljs-comment"># Starting the namenode</span>
<span class="hljs-string">hdfs</span> <span class="hljs-bullet">--daemon</span> <span class="hljs-string">start</span> <span class="hljs-string">namenode</span>
<span class="hljs-comment"># Starting the secondary namenode</span>
<span class="hljs-string">hdfs</span> <span class="hljs-bullet">--daemon</span> <span class="hljs-string">start</span> <span class="hljs-string">secondarynamenode</span>
<span class="hljs-comment"># Starting a datanode (do this on the datanode)</span>
<span class="hljs-string">hdfs</span> <span class="hljs-bullet">--daemon</span> <span class="hljs-string">start</span> <span class="hljs-string">datanode</span>


<span class="hljs-comment"># Starting YARN</span>
<span class="hljs-string">start-yarn.sh</span>

<span class="hljs-comment"># Starting YARN + HDFS</span>
<span class="hljs-string">start-all.sh</span>


<span class="hljs-comment"># Starting the MapReduce History server</span>
<span class="hljs-string">mapred</span> <span class="hljs-bullet">--daemon</span> <span class="hljs-string">start</span> <span class="hljs-string">historyserver</span>


<span class="hljs-comment"># Starting the Spark Cluster master</span>
<span class="hljs-string">$SPARK_HOME/sbin/start-master.sh</span>

<span class="hljs-comment"># Starting the Spark Cluster workers</span>
<span class="hljs-string">$SPARK_HOME/sbin/start-workers.sh</span>

<span class="hljs-comment"># Starting the Spark Cluster master + workers</span>
<span class="hljs-string">$SPARK_HOME/sbin/start-all.sh</span>
</code></pre><p>The table below shows some of the default ports for the running services.</p><center>
<table>
<thead>
<tr>
<th>Service</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS namenode</td>
<td>9870</td>
</tr>
<tr>
<td>HDFS datanode</td>
<td>9864</td>
</tr>
<tr>
<td>HDFS secondary namenode</td>
<td>9868</td>
</tr>
<tr>
<td>YARN resource manager</td>
<td>8088</td>
</tr>
<tr>
<td>YARN node manager</td>
<td>8042</td>
</tr>
<tr>
<td>Spark master</td>
<td>8080</td>
</tr>
<tr>
<td>Spark jobs per application</td>
<td>4040</td>
</tr>
<tr>
<td>Spark history server</td>
<td>18080</td>
</tr>
<tr>
<td>MapReduce history server</td>
<td>19888</td>
</tr>
</tbody>
</table>
</center><ol start="5">
<li>When you are done. You can run the script <code>remove_containers.sh</code> <strong>(CAUTION)</strong> which will remove all containers whose name contains <code>cluster</code>. <strong>Be careful</strong>, if you have any containers that contains <code>cluster</code> in its name, will also be removed.</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-string">bash</span> <span class="hljs-string">remove_containers.sh</span>
</code></pre><p>Or you can remove containers one by one if you do not use this script.</p><h3 id="Optional-Install-PySpark-on-HDP-Sandbox"><a class="anchor hidden-xs" href="#Optional-Install-PySpark-on-HDP-Sandbox" title="Optional-Install-PySpark-on-HDP-Sandbox"><i class="fa fa-link"></i></a>[Optional] Install PySpark on HDP Sandbox</h3><p>If you have installed Python and pip, then you just need to run the following command:</p><pre><code class="yaml hljs"><span class="hljs-string">pip2</span> <span class="hljs-string">install</span> <span class="hljs-string">pyspark</span>
</code></pre><p>Otherwise, you need to return to the install them.</p><div class="alert alert-danger">
<p><strong>Note:</strong> You can use Python3.6 to write Spark applications if you configured it properly in HDP Sandbox.</p>
</div><h2 id="Running-Spark-applications"><a class="anchor hidden-xs" href="#Running-Spark-applications" title="Running-Spark-applications"><i class="fa fa-link"></i></a>Running Spark applications</h2><p>You can execute PySpark statements in interactive mode on the shell <code>pyspark</code> or you can write them in Pyhton file and submit it to Spark using the tool <code>spark-submit</code>. In addition, Spark comes with a set of ready-to-execute examples of Spark applications.</p><h3 id="pyspark-shell"><a class="anchor hidden-xs" href="#pyspark-shell" title="pyspark-shell"><i class="fa fa-link"></i></a>pyspark shell</h3><p>You can open a spark shell session by running the shell <code>pyspark</code> for Python, <code>spark-shell</code> for Scala, <code>sparkR</code> for R language.</p><pre><code class="wrap python hljs">pyspark
</code></pre><p><img src="https://i.imgur.com/H0zwDvA.png" alt="" class="md-image md-image"></p><p>The pyspark shell starts a Spark application and initiates/gets the Spark context <code>sc</code> (<code>spark</code> for Spark session) which internally creates a Web UI with URL localhost:4040 (or next port if 4040 is not free). By default, it uses local[*] as master (this means that Spark will run jobs on the same machine using all CPU cores). It also displays Spark, and Python versions.</p><div class="alert alert-warning">
<p><strong>Note:</strong> If you are running two or more Spark applications simultaneously, then the next ports to 4040 (e.g. 4041, 4042, …etc) are not published in Docker container to the host machine and you can set the port for Spark app web UI manually to one of the free ports such as 4042 as follows:</p>
<pre><code class="sh hljs">docker run --name pyspark -it -<span class="hljs-selector-tag">p</span> <span class="hljs-number">4042</span>:<span class="hljs-number">4042</span> spark:python3 /opt/spark/bin/pyspark --conf spark<span class="hljs-selector-class">.ui</span><span class="hljs-selector-class">.port</span>=<span class="hljs-number">4042</span>
</code></pre>
</div><p>On <code>pyspark</code> shell, you can write only Python code.</p><p><img src="https://i.imgur.com/JE9o6YR.png" alt="" class="md-image md-image"></p><div class="alert alert-info">
<p>You can exit the <code>pyspark</code> shell by executing statement <code>exit()</code> like in a Python shell.</p>
</div><p>Whereas <code>spark-shell</code> command runs a Spark shell for Scala.</p><pre><code class="shell hljs">spark-shell
</code></pre><p><img src="/uploads/29030be24713ff2f56eeb3d03.png" alt="" class="md-image md-image"></p><h3 id="spark-submit"><a class="anchor hidden-xs" href="#spark-submit" title="spark-submit"><i class="fa fa-link"></i></a>spark-submit</h3><p>We can use this tool for non-interactive mode of data anlysis. This command accepts the path for the spark application.</p><p>Let’s write our simple application in <code>app.py</code> and execute it using <code>spark-submit</code>.</p><pre><code class="python hljs"><span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSession
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd


spark = SparkSession\
    .builder\
    .appName(<span class="hljs-string">"PythonSparkApp"</span>)\
    .getOrCreate()    

sc = spark.sparkContext

sc.setLogLevel(<span class="hljs-string">"OFF"</span>) <span class="hljs-comment"># WARN, FATAL, INFO</span>

data = [(<span class="hljs-string">'hello'</span>, <span class="hljs-number">1</span>), (<span class="hljs-string">'world'</span>, <span class="hljs-number">2</span>)]
print(pd.DataFrame(data))

spark.createDataFrame(data).show()

rdd = sc.parallelize(data)
print(rdd.collect())
</code></pre><p>Now you can submit this app using <code>spark-submit</code></p><pre><code class="yaml hljs"><span class="hljs-comment"># Submit the application main.py with default configs</span>
<span class="hljs-string">spark-submit</span> <span class="hljs-string">app.py</span>
<span class="hljs-comment"># master is local[*]</span>

<span class="hljs-comment"># Submit the application main.py on the YARN cluster where the driver program will run in the client machine</span>
<span class="hljs-string">spark-submit</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">    -</span><span class="hljs-bullet">-master</span> <span class="hljs-string">yarn</span>
    <span class="hljs-string">app.p</span>


<span class="hljs-comment"># Run Spark app example 'SparkPi' to calculate the value of π (pi) with the Monte-Carlo method for 10 iterations locally on 8 cores</span>
<span class="hljs-string">spark-submit</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-class</span> <span class="hljs-string">org.apache.spark.examples.SparkPi</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-master</span> <span class="hljs-string">local[8]</span> <span class="hljs-string">\</span>
  <span class="hljs-string">/path/to/examples.jar</span> <span class="hljs-string">\</span>
  <span class="hljs-number">10</span>
<span class="hljs-comment"># See note below.</span>


<span class="hljs-comment"># Run Spark app example 'wordcount.py' with the name "my app" to calculate the number of words in the file `/sparkdata/movies.csv` and run the Spark Uu=i web on the custom port 4242 on Yarn cluster where the driver program can use 2G memory and 4 cores and the application will run on 8 executors where each executor process will have 2G and 4 CPU cores.</span>
<span class="hljs-string">spark-submit</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-name</span> <span class="hljs-string">"my app"</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-master</span> <span class="hljs-string">yarn</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-executor-memory</span> <span class="hljs-number">2</span><span class="hljs-string">G</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-executor-cores</span> <span class="hljs-number">4</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-driver-memory</span> <span class="hljs-number">2</span><span class="hljs-string">G</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-driver-cores</span> <span class="hljs-number">4</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-num-executors</span> <span class="hljs-number">8</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-conf</span> <span class="hljs-string">spark.ui.port=4242</span> <span class="hljs-string">\</span>
  <span class="hljs-string">/usr/hdp/current/spark2-client/examples/src/main/python/wordcount.py</span> <span class="hljs-string">\</span>
  <span class="hljs-string">/sparkdata/movies.csv</span>
</code></pre><p>The general template for <code>spark-submit</code> tool:</p><pre><code class="yaml hljs"><div class="wrapper"><div class="gutter linenumber"><span data-linenumber="1"></span>
<span data-linenumber="2"></span>
<span data-linenumber="3"></span>
<span data-linenumber="4"></span>
<span data-linenumber="5"></span>
<span data-linenumber="6"></span>
<span data-linenumber="7"></span>
<span data-linenumber="8"></span>
<span data-linenumber="9"></span></div><div class="code"><span class="hljs-string">spark-submit</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-name</span> <span class="hljs-string">"app name"</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-class</span> <span class="hljs-string">&lt;main-class&gt;</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-master</span> <span class="hljs-string">&lt;master-url&gt;</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-deploy-mode</span> <span class="hljs-string">&lt;deploy-mode&gt;</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">  -</span><span class="hljs-bullet">-conf</span> <span class="hljs-string">&lt;key&gt;=&lt;value&gt;</span> <span class="hljs-string">\</span>
  <span class="hljs-string">...</span> <span class="hljs-comment"># other options</span>
  <span class="hljs-string">&lt;application-jar&gt;</span> <span class="hljs-string">\</span>
  <span class="hljs-string">[application-arguments]</span>
</div></div></code></pre><p>Some of the commonly used options are:</p><ul>
<li><code>--class</code>: The entry point for your application (e.g. org.apache.spark.examples.SparkPi)</li>
<li><code>--master</code>: The master URL for the cluster (e.g. spark://23.195.26.187:7077)</li>
<li><code>--deploy-mode</code>: Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) (default: client). This option will be explained next lab.</li>
<li><code>--conf</code>: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. <code>--conf &lt;key&gt;=&lt;value&gt; --conf &lt;key2&gt;=&lt;value2&gt;</code>)</li>
<li><code>application-jar</code>: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an hdfs:// path or a file:// path that is present on all nodes.</li>
<li><code>application-arguments</code>: Arguments passed to the main method of your main class, if any</li>
</ul><h2 id="Python-Package-Management-in-PySpark-apps"><a class="anchor hidden-xs" href="#Python-Package-Management-in-PySpark-apps" title="Python-Package-Management-in-PySpark-apps"><i class="fa fa-link"></i></a>Python Package Management in PySpark apps</h2><p>When you want to run your PySpark application on a cluster such as YARN, you need to make sure that your code and all used libraries are available on the executors that actually running on remote machines.</p><p>Let’s try to create a PySpark app that imports <code>pandas</code> package and uses a custom module <code>backend.py</code> as a backend.</p><pre><code class="wrap python hljs"><span class="hljs-comment"># /app/app.py</span>
<span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSession
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd


spark = SparkSession\
    .builder\
    .appName(<span class="hljs-string">"PythonPackageManagementPySparkApp"</span>)\
    .getOrCreate()    

sc = spark.sparkContext
sc.setLogLevel(<span class="hljs-string">"OFF"</span>) <span class="hljs-comment"># WARN, FATAL, INFO</span>

data = [(<span class="hljs-string">'hello'</span>, <span class="hljs-number">1</span>), (<span class="hljs-string">'world'</span>, <span class="hljs-number">2</span>)]
print(pd.DataFrame(data))


spark.createDataFrame(data).show()


rdd = sc.parallelize(data)
print(rdd.collect())

<span class="hljs-keyword">from</span> backend <span class="hljs-keyword">import</span> some_f
some_f(spark)

</code></pre><pre><code class="wrap python hljs"><span class="hljs-comment"># /app/backend.py</span>
<span class="hljs-keyword">import</span> pandas


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">some_f</span><span class="hljs-params">(session)</span>:</span>
    df = pandas.DataFrame()
    print(<span class="hljs-string">f"hello from backend where df.shape is <span class="hljs-subst">{df.shape}</span>"</span>)
    print(pandas.__version__)
    session.createDataFrame(df).show()
</code></pre><p>PySpark allows to upload Python files (.py), zipped Python packages (.zip), and Egg files (.egg) to the executors by setting the configuration setting <code>spark.submit.pyFiles</code>, setting the option <code>--py-files</code> in <code>spark-submit</code>, or directly calling <code>pyspark.SparkContext.addPyFile()</code> in the application. This is the case for custom modules/files but what about packages and virtual environments. You can actually use the python package <code>venv-pack</code> to package your virtual environment and pass it to <code>spark-submit</code> using the option <code>archives</code>.</p><p>Here I have a demo on using a <code>backend.py</code> custom module and <code>pandas</code> package in my application.</p><ol>
<li>Create a virtual environment.</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-string">python3</span> <span class="hljs-bullet">-m</span> <span class="hljs-string">venv</span> <span class="hljs-string">.venv</span>
<span class="hljs-string">source</span> <span class="hljs-string">.venv/bin/activate</span>
</code></pre><ol start="2">
<li>Add the packages that you want to the virtual environment.</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">pandas</span>
<span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">venv-pack</span>
</code></pre><ol start="3">
<li>Package the environment using <code>venv-pack</code></li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-comment"># make sure that you </span>
<span class="hljs-string">venv-pack</span> <span class="hljs-bullet">-o</span> <span class="hljs-string">/app/.venv.tar.gz</span>
</code></pre><ol start="4">
<li>Configure the environment variables for PySpark driver and excutors.</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-comment"># Python of the driver (/app/.venv/bin/python)</span>
<span class="hljs-string">export</span> <span class="hljs-string">PYSPARK_DRIVER_PYTHON=$(which</span> <span class="hljs-string">python)</span> 

<span class="hljs-comment"># Python of the excutor (./.venv/bin/python)</span>
<span class="hljs-string">export</span> <span class="hljs-string">PYSPARK_PYTHON=./.venv/bin/python</span>
</code></pre><ol start="5">
<li>Run the application via <code>spark-submit</code> in client mode.</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-string">spark-submit</span> <span class="hljs-bullet">--master</span> <span class="hljs-string">yarn</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">-</span><span class="hljs-bullet">-conf</span> <span class="hljs-string">spark.yarn.appMasterEnv.PYSPARK_PYTHON=./.venv/bin/python</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">-</span><span class="hljs-bullet">-deploy-mode</span> <span class="hljs-string">client</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">-</span><span class="hljs-bullet">-archives</span> <span class="hljs-string">/app/.venv.tar.gz#.venv</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">-</span><span class="hljs-bullet">-py-files</span> <span class="hljs-string">/app/backend.py</span> <span class="hljs-string">\</span>
<span class="hljs-string">/app/app.py</span>
</code></pre><ol start="6">
<li>You can also run the application in cluster mode as follows:</li>
</ol><pre><code class="wrap yaml hljs"><span class="hljs-string">unset</span> <span class="hljs-string">PYSPARK_DRIVER_PYTHON</span>

<span class="hljs-string">spark-submit</span> <span class="hljs-bullet">--master</span> <span class="hljs-string">yarn</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">-</span><span class="hljs-bullet">-conf</span> <span class="hljs-string">spark.yarn.appMasterEnv.PYSPARK_PYTHON=./.venv/bin/python</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">-</span><span class="hljs-bullet">-deploy-mode</span> <span class="hljs-string">cluster</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">-</span><span class="hljs-bullet">-archives</span> <span class="hljs-string">/app/.venv.tar.gz#.venv</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">-</span><span class="hljs-bullet">-py-files</span> <span class="hljs-string">/app/backend.py</span> <span class="hljs-string">\</span>
<span class="hljs-string">/app/app.py</span>
</code></pre><h1 id="Spark-Web-UI"><a class="anchor hidden-xs" href="#Spark-Web-UI" title="Spark-Web-UI"><i class="fa fa-link"></i></a>Spark Web UI</h1><p>Apache Spark provides a suite of web user interfaces (UIs) that you can use to monitor the status and resource consumption of your Spark cluster.</p><h2 id="Jobs-Tab"><a class="anchor hidden-xs" href="#Jobs-Tab" title="Jobs-Tab"><i class="fa fa-link"></i></a>Jobs Tab</h2><p>The Jobs tab displays a summary page of all jobs in the Spark application and a details page for each job. The summary page shows high-level information, such as the status, duration, and progress of all jobs and the overall event timeline. When you click on a job on the summary page, you see the details page for that job. The details page further shows the event timeline, DAG visualization, and all stages of the job.</p><p>The information that is displayed in this section is:</p><ul>
<li>User: Current Spark user</li>
<li>Total uptime: Time since Spark application started</li>
<li>Scheduling mode: See job scheduling</li>
<li>Number of jobs per status: Active, Completed, Failed</li>
</ul><center>
<p><img src="/uploads/29030be24713ff2f56eeb3d04.png" alt="" width="200" height="200" class="md-image md-image"></p>
</center><ul>
<li>Event timeline: Displays in chronological order the events related to the executors (added, removed) and the jobs</li>
</ul><center>
<p><img src="https://spark.apache.org/docs/latest/img/AllJobsPageDetail2.png" alt="" width="600" height="200" class="md-image md-image"></p>
</center><ul>
<li>Details of jobs grouped by status: Displays detailed information of the jobs including Job ID, description (with a link to detailed job page), submitted time, duration, stages summary and tasks progress bar</li>
</ul><center>
<p><img src="https://spark.apache.org/docs/latest/img/AllJobsPageDetail3.png" alt="" width="1200" height="200" class="md-image md-image"></p>
</center><h3 id="Job-details"><a class="anchor hidden-xs" href="#Job-details" title="Job-details"><i class="fa fa-link"></i></a>Job details</h3><p>When you click on a specific job, you can see the detailed information of this job. This page displays the details of a specific job identified by its job ID.</p><ul>
<li>Job Status: (running, succeeded, failed)</li>
<li>Number of stages per status (active, pending, completed, skipped, failed)</li>
<li>Associated SQL Query: Link to the sql tab for this job</li>
<li>Event timeline: Displays in chronological order the events related to the executors (added, removed) and the stages of the job</li>
</ul><h2 id="Stages-Tab"><a class="anchor hidden-xs" href="#Stages-Tab" title="Stages-Tab"><i class="fa fa-link"></i></a>Stages Tab</h2><p>The Stages tab displays a summary page that shows the current state of all stages of all jobs in the Spark application.</p><p>At the beginning of the page is the summary with the count of all stages by status (active, pending, completed, skipped, and failed)</p><center>
<p><img src="https://spark.apache.org/docs/latest/img/AllStagesPageDetail1.png" alt="" width="200" height="100" class="md-image md-image"></p>
</center><p>After that are the details of stages per status (active, pending, completed, skipped, failed). In active stages, it’s possible to kill the stage with the kill link. Only in failed stages, failure reason is shown. Task detail can be accessed by clicking on the description.</p><center>
<p><img src="https://spark.apache.org/docs/latest/img/AllStagesPageDetail3.png" alt="" class="md-image md-image"></p>
</center><h3 id="Stage-details"><a class="anchor hidden-xs" href="#Stage-details" title="Stage-details"><i class="fa fa-link"></i></a>Stage details</h3><p>The stage details page begins with information like total time across all tasks, Locality level summary, Shuffle Read Size / Records and Associated Job IDs.</p><center>
<p><img src="https://spark.apache.org/docs/latest/img/AllStagesPageDetail4.png" alt="" width="250" height="100" class="md-image md-image"></p>
</center><p>There is also a visual representation of the directed acyclic graph (DAG) of this stage, where vertices represent the RDDs or DataFrames and the edges represent an operation to be applied. Nodes are grouped by operation scope in the DAG visualization and labelled with the operation scope name (BatchScan, WholeStageCodegen, Exchange, etc). Notably, Whole Stage Code Generation operations are also annotated with the code generation id. For stages belonging to Spark DataFrame or SQL execution, this allows to cross-reference Stage execution details to the relevant details in the Web-UI SQL Tab page where SQL plan graphs and execution plans are reported.</p><center>
<p><img src="https://spark.apache.org/docs/latest/img/AllStagesPageDetail5.png" alt="" width="400" height="400" class="md-image md-image"></p>
</center><div class="alert alert-warning">
<p>You can find more info about the web UI from <a href="https://spark.apache.org/docs/latest/web-ui.html" target="_blank" rel="noopener">here</a>.</p>
</div><h1 id="Spark-Core"><a class="anchor hidden-xs" href="#Spark-Core" title="Spark-Core"><i class="fa fa-link"></i></a>Spark Core</h1><p>RDD is a fundamental building block of Spark. RDDs are immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster.</p><p>In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process.</p><p>Additionally, RDDs provide data abstraction of partitioning and distribution of the data designed to run computations in parallel on several nodes, while doing transformations on RDD we do not have to worry about the parallelism as Spark by default provides.</p><h2 id="Spark-RDD-Operations"><a class="anchor hidden-xs" href="#Spark-RDD-Operations" title="Spark-RDD-Operations"><i class="fa fa-link"></i></a>Spark RDD Operations</h2><p>There are two main types of Spark operations: Transformations and Actions.</p><p><img src="https://i.imgur.com/5Rzg8rU.png" alt="" class="md-image md-image"></p><h2 id="Spark-RDD-Transformations"><a class="anchor hidden-xs" href="#Spark-RDD-Transformations" title="Spark-RDD-Transformations"><i class="fa fa-link"></i></a>Spark RDD Transformations</h2><p>Spark RDD Transformations are functions that take an RDD as the input and produce one or many RDDs as the output. They do not change the input RDD (since RDDs are immutable), but always produce one or more new RDDs by applying the computations they represent e.g. map(), filter(), reduceByKey() etc.</p><p>Spark supports lazy evaluation and when you apply the transformation on any RDD it will not perform the operation immediately. It will create a DAG(Directed Acyclic Graph) using 1) the applied operation, 2) source RDD and 3) function used for transformation. It will keep on building this graph using the references till you apply any action operation on the last lined up RDDs. That is why the transformations in Spark are lazy.</p><p>Transformations construct a new RDD from a previous one.</p><p><img src="https://i.imgur.com/GJgpmBr.png" alt="" class="md-image md-image"></p><p>For example, we can build a simple Spark application for counting the words in a text file. Let’s write a simple <code>wordcount.py</code> application and run it using PySpark. This application accepts any text file and returns the number of words in the text, so it is basically a word counter.</p><pre><code class="python hljs"><span class="hljs-keyword">import</span> pyspark

<span class="hljs-keyword">import</span> sys

<span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSession

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">if</span> len(sys.argv) != <span class="hljs-number">2</span>:
        print(<span class="hljs-string">"Usage: wordcount &lt;file&gt;"</span>, file=sys.stderr)
        sys.exit(<span class="hljs-number">-1</span>)

    spark = SparkSession\
        .builder\
        .appName(<span class="hljs-string">"PythonWordCount"</span>)\
        .getOrCreate()

    lines = spark.read.text(sys.argv[<span class="hljs-number">1</span>]).rdd.map(<span class="hljs-keyword">lambda</span> r: r[<span class="hljs-number">0</span>])
    counts = lines.flatMap(<span class="hljs-keyword">lambda</span> x: x.split(<span class="hljs-string">' '</span>)) \
                  .map(<span class="hljs-keyword">lambda</span> x: (x, <span class="hljs-number">1</span>)) \
                  .reduceByKey(<span class="hljs-keyword">lambda</span> x, y: x + y)
    output = counts.collect()
    <span class="hljs-keyword">for</span> (word, count) <span class="hljs-keyword">in</span> output:
        print(<span class="hljs-string">"%s: %i"</span> % (word, count))

    spark.stop()
</code></pre><p>A spark job of two stages needs to be created (we do not create stages manually but we define a pipeline for which the stages will be determined):</p><ul>
<li>
<p>Stage 1</p>
<ul>
<li>Read the data from the files
<ul>
<li>input: text file</li>
<li>output: RDD1</li>
</ul>
</li>
<li>Split the sentences in the RDD into words
<ul>
<li>input: RDD1</li>
<li>output: RDD2</li>
<li>operation: flatMap(split the words by space)</li>
<li>The elements of RDD are only values.</li>
</ul>
</li>
<li>Initialize the counters to 1 for each word.
<ul>
<li>input: RDD2</li>
<li>output: RDD3</li>
<li>operation: map(initialize the counters to 1 for each word)</li>
<li>The elements of RDD are key-value pairs.
<ul>
<li>PairRDD</li>
</ul>
</li>
</ul>
</li>
<li>Aggregates the counts of words where the group key is the word.
<ul>
<li>input: RDD3</li>
<li>ouput: RDD4</li>
<li>operation: reduceByKey(aggregates the counts based on the word which is the key in the key-value pair)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Stage 2</p>
<ul>
<li>Print the word counts to the screen
<ul>
<li>input: RDD4</li>
<li>output: RDD5</li>
<li>operation: foreach(prints the words with their counts)</li>
</ul>
</li>
</ul>
</li>
</ul><p><img src="/uploads/f3ca432d9a90db31c0e548500.png" alt="" class="md-image md-image"></p><p><img src="https://i.imgur.com/zomLW1S.png" alt="" class="md-image md-image"></p><p>There are two types of transformations:<br>
<strong>Narrow transformation</strong> — In Narrow transformation, all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().</p><p><img src="https://i.imgur.com/LFPbHzv.png" alt="" class="md-image md-image"></p><p><strong>Wide transformation</strong> — In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey() and reducebyKey().</p><p><strong>Spark Pair RDDs</strong> are nothing but RDDs containing a key-value pair. Basically, key-value pair (KVP) consists of a two linked data item in it. Here, the key is the identifier, whereas value is the data corresponding to the key value.<br>
Moreover, Spark operations work on RDDs containing any type of objects. However key-value pair RDDs attains few special operations in it. Such as, distributed “shuffle” operations, grouping or aggregating the elements by a key.</p><p>The following Spark transformations accept input as an RDD consisting of single values.<br>
<img src="https://i.imgur.com/RqpWt7h.png" alt="" class="md-image md-image"></p><p>Where the transformations below accept an input as a pair RDD consisting of key-value pairs.<br>
<img src="https://i.imgur.com/DbTTs5o.png" alt="" class="md-image md-image"></p><h2 id="Spark-RDD-Actions"><a class="anchor hidden-xs" href="#Spark-RDD-Actions" title="Spark-RDD-Actions"><i class="fa fa-link"></i></a>Spark RDD Actions</h2><p>Actions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS).</p><p>The following actions are applied on RDDs which contains single values.<br>
<img src="https://i.imgur.com/XOM1KVq.png" alt="" class="md-image md-image"></p><p>Action is one of the ways of sending data from Executer to the driver. Executors are agents that are responsible for executing a task. While the driver is a JVM process that coordinates workers and execution of the task. The following actions are applied on pair RDDs which contain key-value pairs.<br>
<img src="https://i.imgur.com/vdaocQQ.png" alt="" class="md-image md-image"></p><h1 id="Spark-Context"><a class="anchor hidden-xs" href="#Spark-Context" title="Spark-Context"><i class="fa fa-link"></i></a>Spark Context</h1><p>The dataset for this demo is movies data and can be downloaded from the link attached to this document.</p><h2 id="Import-required-packages"><a class="anchor hidden-xs" href="#Import-required-packages" title="Import-required-packages"><i class="fa fa-link"></i></a>Import required packages</h2><pre><code class="python hljs"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> pyspark 
<span class="hljs-keyword">import</span> pymongo
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSession
<span class="hljs-keyword">from</span> os.path <span class="hljs-keyword">import</span> join
</code></pre><h2 id="Create-a-SparkContext"><a class="anchor hidden-xs" href="#Create-a-SparkContext" title="Create-a-SparkContext"><i class="fa fa-link"></i></a>Create a SparkContext</h2><pre><code class="wrap python hljs">
<span class="hljs-comment"># Import SparkSession</span>
<span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSession 

<span class="hljs-comment"># Create SparkSession </span>
spark = SparkSession \
        .builder \
        .appName(<span class="hljs-string">"my spark app"</span>) \
        .master(<span class="hljs-string">"local[*]"</span>) \ <span class="hljs-comment"># local[n] it uses n cores/threads for running spark job</span>
        .getOrCreate()

sc = spark.sparkContext

<span class="hljs-comment"># Or</span>

<span class="hljs-comment"># Import SparkContext and SparkConf</span>
<span class="hljs-keyword">from</span> pyspark <span class="hljs-keyword">import</span> SparkContext, SparkConf

<span class="hljs-comment"># Create SparkContext</span>
conf = SparkConf() \
        .setAppName(<span class="hljs-string">"my spark app"</span>)\
        .setMaster(<span class="hljs-string">"local[*]"</span>)

sc = SparkContext(conf=conf)
spark = SparkSession.builder.config(conf).getOrCreate()
sc = spark.sparkContext

</code></pre><div class="alert alert-warning">
<p>Here we are using the local machine as the resource manager with as many as threads/cores as it has.</p>
</div><div class="alert alert-warning">
<p>A Spark Driver is an application that creates a <code>SparkContext</code> for executing one or more jobs in the cluster. It allows your Spark/PySpark application to access Spark Cluster with the help of Resource Manager.</p>
<p>When you create a <code>SparkSession</code> object, <code>SparkContext</code> is also created and can be retrieved using <code>spark.sparkContext</code>. <code>SparkContext</code> will be created only once for an application; even if you try to create another SparkContext, it still returns existing SparkContext.</p>
</div><h1 id="Spark-RDD"><a class="anchor hidden-xs" href="#Spark-RDD" title="Spark-RDD"><i class="fa fa-link"></i></a>Spark RDD</h1><p>There are multiple ways to create RDDs in PySpark.</p><h3 id="1-using-parallelize-function"><a class="anchor hidden-xs" href="#1-using-parallelize-function" title="1-using-parallelize-function"><i class="fa fa-link"></i></a>1. using <code>parallelize()</code> function</h3><pre><code class="python hljs">data = [(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>, <span class="hljs-string">'a b c'</span>), (<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>, <span class="hljs-string">'d e f'</span>), (<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-string">'g h i'</span>)]
rdd = sc.parallelize(data) <span class="hljs-comment"># Spark RDD creation</span>
<span class="hljs-comment"># rdd = sc.parallelize(data, n) # n is minimum number of partitions</span>
</code></pre><p>This function will split the dataset into multiple partitions. You can get number of partitions by using the function <code>&lt;rdd&gt;.getNumParititions()</code>.</p><h3 id="2-using-textFile-or-wholeTextFiles-functions"><a class="anchor hidden-xs" href="#2-using-textFile-or-wholeTextFiles-functions" title="2-using-textFile-or-wholeTextFiles-functions"><i class="fa fa-link"></i></a>2. using <code>textFile</code> or <code>wholeTextFiles</code> functions</h3><h4 id="Read-from-a-local-file"><a class="anchor hidden-xs" href="#Read-from-a-local-file" title="Read-from-a-local-file"><i class="fa fa-link"></i></a>Read from a local file</h4><pre><code class="python hljs">path = <span class="hljs-string">"file:///data/movies.csv"</span>

rdd = sc.textFile(path) <span class="hljs-comment"># a spark rdd</span>
</code></pre><p>The file <code>movies.csv</code> is uploaded to the local file system and stored in the folder <code>/sparkdata</code>.</p><h4 id="Read-from-HDFS"><a class="anchor hidden-xs" href="#Read-from-HDFS" title="Read-from-HDFS"><i class="fa fa-link"></i></a>Read from HDFS</h4><pre><code class="wrap python hljs">path = <span class="hljs-string">"hdfs://localhost:9000/data/movies.csv"</span>
<span class="hljs-comment"># path = "/data/movies.csv"</span>
    
rdd3 = sc.textFile(path) <span class="hljs-comment"># a spark rdd</span>
</code></pre><p>The file <code>movies.csv</code> is uploaded to HDFS and stored in the folder <code>/data</code>.</p><div class="alert alert-info">
<p>The function <code>wholeTextFiles</code> will read all files in the directory and each file will be read as a single row in the RDD whereas the function <code>textFile</code> will read each line of the file as a row in the RDD.</p>
</div><div class="alert alert-warning">
<p>When we use <code>parallelize()</code> or <code>textFile()</code> or <code>wholeTextFiles()</code> methods of <code>SparkContxt</code> to initiate RDD, it automatically splits the data into partitions based on resource availability. when you run it on a local machine it would create partitions as the same number of cores available on your system.</p>
</div><h2 id="Create-empty-RDD"><a class="anchor hidden-xs" href="#Create-empty-RDD" title="Create-empty-RDD"><i class="fa fa-link"></i></a>Create empty RDD</h2><pre><code class="wrap python hljs">rdd = sc.emptyRDD()

rdd = sc.paralellize([], <span class="hljs-number">10</span>)
</code></pre><p>This will create an empty RDD with 10 partitions.</p><h2 id="Repartition-and-Coalesce"><a class="anchor hidden-xs" href="#Repartition-and-Coalesce" title="Repartition-and-Coalesce"><i class="fa fa-link"></i></a>Repartition and Coalesce</h2><p>Sometimes we may need to repartition the RDD, PySpark provides two ways to repartition; first using <code>repartition()</code> method which shuffles data from all nodes also called full shuffle and second <code>coalesce()</code> method which shuffle data from minimum nodes, for examples if you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes.</p><pre><code class="wrap python hljs">rdd = sc.parallelize(range(<span class="hljs-number">1</span>,<span class="hljs-number">100</span>), <span class="hljs-number">10</span>)

print(rdd.getNumPartitions())
print(rdd.collect())

rdd2 = rdd.repartition(<span class="hljs-number">4</span>)
print(rdd2.getNumPartitions())
print(rdd2.collect())

rdd3 = rdd.coalesce(<span class="hljs-number">4</span>)
print(rdd3.getNumPartitions())
print(rdd3.collect())
</code></pre><p>Note that <code>repartition()</code> method is a very expensive operation as it shuffles data from all nodes in a cluster. Both functions return RDD.</p><div class="alert alert-info">
<ul>
<li><code>collect</code> is a Spark action and returns all elements of the RDD.</li>
<li><code>repartition</code> and <code>coalesce</code> are considered as transformations in Spark.</li>
</ul>
</div><h2 id="PySpark-RDD-Transformations"><a class="anchor hidden-xs" href="#PySpark-RDD-Transformations" title="PySpark-RDD-Transformations"><i class="fa fa-link"></i></a>PySpark RDD Transformations</h2><p>Transformations are lazy operations, instead of updating an RDD, these operations return another RDD.</p><h3 id="map-and-flatMap"><a class="anchor hidden-xs" href="#map-and-flatMap" title="map-and-flatMap"><i class="fa fa-link"></i></a>map and flatMap</h3><p><code>map</code> operation returns a new RDD by applying a function to each element of this RDD. <code>flatMap</code> applies the map operation and then flattens the RDD rows. We use this function when you the map operation returns a list of values and flattening will convert the list of list of values into list of values.</p><pre><code class="wrap python hljs"><span class="hljs-comment"># Read file</span>
rdd1 = sc.textFile(<span class="hljs-string">"movies.csv"</span>)
rdd1.take(<span class="hljs-number">10</span>)

<span class="hljs-comment"># tokenize</span>
rdd2 = rdd1.flatMap(<span class="hljs-keyword">lambda</span> x : x.split(<span class="hljs-string">","</span>))
rdd2.take(<span class="hljs-number">10</span>)

<span class="hljs-comment"># Or</span>
<span class="hljs-comment"># def f(x): </span>
<span class="hljs-comment">#     return x.split(",")</span>
<span class="hljs-comment"># rdd2 = rdd1.flatMap(f)</span>
<span class="hljs-comment"># rdd2.take(10)</span>

<span class="hljs-comment"># Remove the additional spaces</span>
rdd3 = rdd2.map(<span class="hljs-keyword">lambda</span> x : x.strip())
rdd3.take(<span class="hljs-number">10</span>)
</code></pre><div class="alert alert-info">
<p><code>take(k)</code> is a Spark action and returns the first <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4" style="width: 0.649em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.541em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1000.54em, 2.481em, -999.997em); top: -2.314em; left: 0em;"><span class="mrow" id="MathJax-Span-5"><span class="mi" id="MathJax-Span-6" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.32em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">k</script></span> elements of the RDD.</p>
</div><h3 id="filter"><a class="anchor hidden-xs" href="#filter" title="filter"><i class="fa fa-link"></i></a>filter</h3><p>Returns a new RDD after applying filter function on source dataset.</p><pre><code class="wrap python hljs"><span class="hljs-comment"># Returns only values which are digits</span>
rdd4 = rdd3.filter(<span class="hljs-keyword">lambda</span> x : str(x).isdigit())

print(rdd4.count())
</code></pre><div class="alert alert-info">
<p><code>count</code> is a Spark action and returns the number of elements in the RDD.</p>
</div><h3 id="distinct"><a class="anchor hidden-xs" href="#distinct" title="distinct"><i class="fa fa-link"></i></a>distinct</h3><p>Returns a new RDD after eliminating all duplicated elements…</p><pre><code class="wrap python hljs"><span class="hljs-comment"># Returns unique number of values which are only digits</span>
rdd5 = rdd4.distinct()

print(rdd5.count())
</code></pre><h3 id="sample"><a class="anchor hidden-xs" href="#sample" title="sample"><i class="fa fa-link"></i></a>sample</h3><p>Return a sampled subset of this RDD.</p><pre><code class="wrap python hljs">rdd6 = rdd5.sample(withReplacement=<span class="hljs-literal">False</span>, fraction=<span class="hljs-number">0.6</span>, seed=<span class="hljs-number">0</span>)

print(rdd6.count())
</code></pre><h3 id="randomSplit"><a class="anchor hidden-xs" href="#randomSplit" title="randomSplit"><i class="fa fa-link"></i></a>randomSplit</h3><p>Splits the RDD by the weights specified in the argument. For example rdd.randomSplit(0.7,0.3)</p><pre><code class="wrap python hljs">rdd7, rdd8 = rdd1.randomSplit(weights=[<span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>], seed=<span class="hljs-number">0</span>)

print(rdd7.count())
print(rdd8.count())
</code></pre><h3 id="mapPartitions-and-mapPartitionsWithIndex"><a class="anchor hidden-xs" href="#mapPartitions-and-mapPartitionsWithIndex" title="mapPartitions-and-mapPartitionsWithIndex"><i class="fa fa-link"></i></a>mapPartitions and mapPartitionsWithIndex</h3><p><code>mapPartitions</code> is similar to <code>map</code>, but executes transformation function on each partition, This gives better performance than <code>map</code> function. <code>mapPartitionsWithIndex</code> returns a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition. They both should return a generator.</p><pre><code class="wrap python hljs"><span class="hljs-comment"># a generator function</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f9</span><span class="hljs-params">(iter)</span>:</span>
    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> iter:
        <span class="hljs-keyword">yield</span> (v, <span class="hljs-number">1</span>)

rdd9 = rdd3.mapPartitions(f9)

print(rdd9.take(<span class="hljs-number">10</span>))

<span class="hljs-comment"># a generator function</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f10</span><span class="hljs-params">(index, iter)</span>:</span>
    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> iter:
        <span class="hljs-keyword">yield</span> (v, index)

rdd10 = rdd3.mapPartitionsWithIndex(f10)
print(rdd10.take(<span class="hljs-number">10</span>))
</code></pre><h3 id="sortBy-and-groupBy"><a class="anchor hidden-xs" href="#sortBy-and-groupBy" title="sortBy-and-groupBy"><i class="fa fa-link"></i></a>sortBy and groupBy</h3><pre><code class="wrap python hljs"><span class="hljs-comment"># Compute the word-digit frequency in the file and show the top 10 words.</span>

<span class="hljs-comment"># Group by all tokens</span>
rdd11 = rdd3.groupBy(<span class="hljs-keyword">lambda</span> x : x)

<span class="hljs-comment"># Calculate the length of the list of token duplicates</span>
rdd12 = rdd11.map(<span class="hljs-keyword">lambda</span> x: (x[<span class="hljs-number">0</span>], len(x[<span class="hljs-number">1</span>])))
<span class="hljs-comment"># or</span>
<span class="hljs-comment"># rdd12 = rd11.mapValues(len)</span>

<span class="hljs-comment"># Sort the results</span>
rdd13 = rdd12.sortBy(<span class="hljs-keyword">lambda</span> x : x[<span class="hljs-number">1</span>], ascending=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Take the first elements of the RDD and display</span>
print(rdd13.take(<span class="hljs-number">10</span>))
</code></pre><p><img src="https://i.imgur.com/7HDMygw.png" alt="" class="md-image md-image"></p><h3 id="sortByKey-and-reduceByKey"><a class="anchor hidden-xs" href="#sortByKey-and-reduceByKey" title="sortByKey-and-reduceByKey"><i class="fa fa-link"></i></a>sortByKey and reduceByKey</h3><pre><code class="wrap python hljs"><span class="hljs-comment"># Compute the digit frequency in the file and show the top 10 words.</span>

<span class="hljs-comment"># Get all digits</span>
rdd14 = rdd3.filter(<span class="hljs-keyword">lambda</span> x: x.isdigit())

<span class="hljs-comment"># Initialize the counters</span>
rdd15 = rdd14.map(<span class="hljs-keyword">lambda</span> x : (x, <span class="hljs-number">1</span>))

<span class="hljs-comment"># Aggregate the counters who have same key which is here a digit</span>
rdd16 = rdd15.reduceByKey(<span class="hljs-keyword">lambda</span> x, y : x+y)

<span class="hljs-comment"># Sort the results</span>
rdd17 = rdd16.sortBy(<span class="hljs-keyword">lambda</span> x : x[<span class="hljs-number">1</span>], ascending=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Take the first elements of the RDD and display</span>
print(rdd17.take(<span class="hljs-number">10</span>))
</code></pre><p><img src="https://i.imgur.com/cXhsMEs.png" alt="" class="md-image md-image"></p><h2 id="PySpark-RDD-Actions"><a class="anchor hidden-xs" href="#PySpark-RDD-Actions" title="PySpark-RDD-Actions"><i class="fa fa-link"></i></a>PySpark RDD Actions</h2><p>RDD Action operations return the values from an RDD to a driver program. In other words, any RDD function that returns non-RDD is considered as an action.</p><h3 id="collect"><a class="anchor hidden-xs" href="#collect" title="collect"><i class="fa fa-link"></i></a>collect</h3><p>returns the complete dataset as an Array.</p><div class="alert alert-danger">
<p>Using the action on a large RDD is dangerous because it will collect all elements of the distributed RDD in one machine (the driver) and this can cause the machine to run out of memory since Spark is an in-memory processing engine.</p>
</div><h3 id="max-min-first-top-take"><a class="anchor hidden-xs" href="#max-min-first-top-take" title="max-min-first-top-take"><i class="fa fa-link"></i></a>max, min, first, top, take</h3><p><code>max</code> returns the maximum value from the dataset whereas <code>min</code> returns the minimum value from the dataset. <code>first</code> returns the first element in the dataset. <code>top</code> returns top <code>n</code> elements from the dataset (after sorting them). <code>take</code> returns the first <code>n</code> elements of the dataset.</p><p><img src="https://i.imgur.com/2OfDP3p.png" alt="" class="md-image md-image"></p><div class="alert alert-info">
<p>The operation <code>take</code> in Spark RDD is the same as <code>head</code> in pandas DataFrame whereas <code>top</code> is interpreted as the first elements after sorting them.</p>
</div><h3 id="count-countByValue"><a class="anchor hidden-xs" href="#count-countByValue" title="count-countByValue"><i class="fa fa-link"></i></a>count, countByValue</h3><p><code>count</code> returns the count of elements in the dataset.</p><div class="alert alert-info">
<p>There are other similar operations. <code>countApprox(timeout, confidence=0.95)</code> which is the approximate version of <code>count()</code> and returns a potentially incomplete result within a timeout, even if not all tasks have finished. <code>countApproxDistinct(relative_accuracy)</code> returns an approximate number of distinct elements in the dataset.</p>
<p><strong>Note:</strong> These operations are used when you have very large dataset which takes a lot of time to get the count.</p>
</div><p><code>countByValue</code>Return a dictionary where the key represents each unique value in the dataset and the value represents count of each value present.</p><pre><code class="wrap python hljs">print(rdd3.countByValue())
</code></pre><p><img src="https://i.imgur.com/hBLLvZe.png" alt="" class="md-image md-image"></p><h3 id="reduce-treeReduce"><a class="anchor hidden-xs" href="#reduce-treeReduce" title="reduce-treeReduce"><i class="fa fa-link"></i></a>reduce, treeReduce</h3><p><code>reduce</code> reduces the elements of the dataset using the specified binary operator. <code>treeReduce</code> reduces the elements of this RDD in a multi-level tree pattern. The output is the same.</p><pre><code class="wrap python hljs">result = rdd3 \
    .map(<span class="hljs-keyword">lambda</span> x : <span class="hljs-number">1</span>) \
    .reduce(<span class="hljs-keyword">lambda</span> x, y : x+y)

resultTree = rdd3 \
    .map(<span class="hljs-keyword">lambda</span> x : <span class="hljs-number">1</span>) \
    .treeReduce(<span class="hljs-keyword">lambda</span> x, y : x+y, <span class="hljs-number">3</span>)


<span class="hljs-comment"># You should get the same results as rdd3.count() operation</span>
<span class="hljs-keyword">assert</span> rdd3.count()==result==resultTree
</code></pre><div class="alert alert-warning">
<p>Here I showed some of the operations, but you can find more in the <a href="https://spark.apache.org/docs/latest/api/python/index.html" target="_blank" rel="noopener">documentation</a>.</p>
</div><h3 id="saveAsTextFile"><a class="anchor hidden-xs" href="#saveAsTextFile" title="saveAsTextFile"><i class="fa fa-link"></i></a>saveAsTextFile</h3><p>Used to save the rdd to an external data store.</p><pre><code class="wrap python hljs">rdd3.saveAsTextFile(<span class="hljs-string">"/root/myrdd"</span>)
</code></pre><p><img src="https://i.imgur.com/qJv2oqR.png" alt="" class="md-image md-image"></p><h2 id="RDD-Persistence"><a class="anchor hidden-xs" href="#RDD-Persistence" title="RDD-Persistence"><i class="fa fa-link"></i></a>RDD Persistence</h2><p>Persistence is useful due to:</p><ul>
<li>Cost efficient – Spark computations are very expensive hence reusing the computations are used to save cost.</li>
<li>Time efficient – Reusing the repeated computations saves lots of time.</li>
<li>Execution time – Saves execution time of the job which allows us to perform more jobs on the same cluster.</li>
</ul><p>We have different levels for storage like memory, disk, serialized, unserialized, repliacted, unreplicated. You can check <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence" target="_blank" rel="noopener">here</a> for the avilable options.</p><h3 id="RDD-Cache"><a class="anchor hidden-xs" href="#RDD-Cache" title="RDD-Cache"><i class="fa fa-link"></i></a>RDD Cache</h3><p>PySpark RDD <code>cache()</code> method by default saves RDD computation to storage level <code>MEMORY_ONLY</code> meaning it will store the data in the JVM heap as unserialized objects.</p><pre><code class="wrap python hljs">cachedRDD = rdd.cache()

cachedRDD.collect()
</code></pre><h3 id="RDD-Persist"><a class="anchor hidden-xs" href="#RDD-Persist" title="RDD-Persist"><i class="fa fa-link"></i></a>RDD Persist</h3><p>PySpark <code>persist()</code> method is used to store the RDD to a specific storage level.</p><pre><code class="wrap python hljs"><span class="hljs-keyword">import</span> pyspark

persistedRDD = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)

persistedRDD.collect()
</code></pre><h3 id="RDD-Unpersist"><a class="anchor hidden-xs" href="#RDD-Unpersist" title="RDD-Unpersist"><i class="fa fa-link"></i></a>RDD Unpersist</h3><p>PySpark automatically monitors every <code>persist()</code> and <code>cache()</code> calls you make and it checks usage on each node and drops persisted data if not used or by using least-recently-used (LRU) algorithm. You can also manually remove using <code>unpersist()</code> method. <code>unpersist()</code> marks the RDD as non-persistent, and remove all blocks for it from memory and disk.</p><pre><code class="wrap python hljs">unpersistedRDD = persistedRDD.unpersist()

unpersistedRDD.collect()
</code></pre><h2 id="Shuffling-in-Spark-engine"><a class="anchor hidden-xs" href="#Shuffling-in-Spark-engine" title="Shuffling-in-Spark-engine"><i class="fa fa-link"></i></a>Shuffling in Spark engine</h2><p>Shuffling is a mechanism Spark to redistribute the data across different executors and even across machines. PySpark shuffling triggers when we perform certain transformation operations like <code>gropByKey()</code>, <code>reduceByKey()</code>, <code>join()</code> on RDDS.</p><p>Shuffling is an expensive operation since it involves the following:</p><ul>
<li>Disk I/O</li>
<li>Data serialization and deserialization</li>
<li>Network I/O</li>
</ul><p>For example, when we perform <code>reduceByKey()</code> operation, PySpark does the following:</p><ol>
<li>Spark engine firstly runs map tasks on all partitions which groups all values for a single key.</li>
<li>The results of the map tasks are kept in memory.</li>
<li>When results do not fit in memory, PySpark stores the data into a disk.</li>
<li>PySpark shuffles the mapped data across partitions, some times it also stores the shuffled data into a disk for reuse when it needs to recalculate.</li>
<li>Run the garbage collection</li>
<li>Finally runs reduce tasks on each partition based on key.</li>
</ol><p>PySpark RDD triggers shuffle and repartition for several operations like <code>repartition()</code>, <code>coalesce()</code>,  <code>groupByKey()</code>, and  <code>reduceByKey()</code>.</p><p>Based on your dataset size, a number of cores and specific memory size, can benefit or harm the Spark shuffling. When you deal with less amount of data, you should typically reduce the number of partitions otherwise you will end up with many partitioned files with less number of records in each partition. which results in running many tasks with lesser data to process.</p><p>On other hand, when you have too much of data and having less number of partitions results in fewer longer running tasks and some times you may also get out of memory error.</p><p>Getting the right size of the shuffle partition is always tricky and takes many runs with different values to achieve the optimized number. This is one of the key properties to look for when you have performance issues on Spark jobs.</p><h2 id="Shared-Variables"><a class="anchor hidden-xs" href="#Shared-Variables" title="Shared-Variables"><i class="fa fa-link"></i></a>Shared Variables</h2><p>When Spark executes transformation using <code>map</code> or <code>reduce</code> operations, It executes the transformations on a remote node by using the variables that are shipped with the tasks and these variables are not sent back to PySpark Driver hence there is no capability to reuse and sharing the variables across tasks. PySpark shared variables solve this problem using the below two techniques. PySpark provides two types of shared variables.</p><ul>
<li>Broadcast variables (read-only shared variable)</li>
<li>Accumulator variables (updatable shared variables)</li>
</ul><h3 id="Broadcast-variables"><a class="anchor hidden-xs" href="#Broadcast-variables" title="Broadcast-variables"><i class="fa fa-link"></i></a>Broadcast variables</h3><p>We can create broadcast variables using the function <code>sc.broadcast</code>. A broadcast variable created with <code>SparkContext.broadcast()</code>. Access its value through <code>value</code>.</p><pre><code class="wrap python hljs">v = sc.broadcast(range(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>))

print(v.value)
</code></pre><h3 id="Accumulator-variables"><a class="anchor hidden-xs" href="#Accumulator-variables" title="Accumulator-variables"><i class="fa fa-link"></i></a>Accumulator variables</h3><p>A shared variable that can be accumulated, i.e., has a commutative and associative <code>add</code> operation. Worker tasks on a Spark cluster can add values to an Accumulator with the <code>+=</code> operator, but only the driver program is allowed to access its value, using value. Updates from the workers get propagated automatically to the driver program.</p><pre><code class="wrap python hljs">acc = sc.accumulator(<span class="hljs-number">0</span>)

acc+=<span class="hljs-number">10</span>
acc.add(<span class="hljs-number">10</span>)

print(acc.value) <span class="hljs-comment"># 20</span>
</code></pre><h1 id="References"><a class="anchor hidden-xs" href="#References" title="References"><i class="fa fa-link"></i></a>References</h1><ul>
<li><a href="https://spark.apache.org" target="_blank" rel="noopener">Apache Spark</a></li>
<li><a href="https://blog.knoldus.com/deep-dive-into-apache-spark-transformations-and-action/" target="_blank" rel="noopener">Deep Dive into Apache Spark Transformations and Action</a></li>
<li><a href="https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/" target="_blank" rel="noopener">Apache Spark RDD vs DataFrame vs DataSet</a></li>
<li><a href="https://sparkbyexamples.com/pyspark-tutorial/" target="_blank" rel="noopener">Spark with Python (PySpark) Tutorial For Beginners</a></li>
<li><a href="https://www.amazon.com/Learning-PySpark-Tomasz-Drabas/dp/1786463709" target="_blank" rel="noopener">Learning PySpark</a></li>
<li><a href="https://spark.apache.org/docs/latest/web-ui.html#storage-tab" target="_blank" rel="noopener">Spark Web UI Guide</a></li>
<li><a href="https://sparkbyexamples.com/spark/spark-submit-command/" target="_blank" rel="noopener">spark-submit examples</a></li>
<li><a href="https://jcristharif.com/venv-pack/spark.html" target="_blank" rel="noopener">venv-pack tutorial</a></li>
</ul></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#Lab-6---Apache-Spark-RDD" title="Lab 6 - Apache Spark RDD">Lab 6 - Apache Spark RDD</a><ul class="nav">
<li class=""><a href="#Datasets" title="Datasets">Datasets</a></li>
<li class=""><a href="#PySpark-on-Colab" title="PySpark on Colab">PySpark on Colab</a></li>
<li class=""><a href="#Readings" title="Readings">Readings</a></li>
</ul>
</li>
<li class=""><a href="#Agenda" title="Agenda">Agenda</a></li>
<li class=""><a href="#Prerequisites" title="Prerequisites">Prerequisites</a></li>
<li class=""><a href="#Objectives" title="Objectives">Objectives</a></li>
<li class=""><a href="#Intro-to-Apache-Spark-review" title="Intro to Apache Spark [review]">Intro to Apache Spark [review]</a><ul class="nav">
<li class=""><a href="#Core-Concepts-in-Spark" title="Core Concepts in Spark">Core Concepts in Spark</a></li>
<li class=""><a href="#Spark-Application-Model" title="Spark Application Model">Spark Application Model</a></li>
<li class=""><a href="#Spark-Execution-Model" title="Spark Execution Model">Spark Execution Model</a></li>
<li class=""><a href="#Spark-Components" title="Spark Components">Spark Components</a></li>
<li class=""><a href="#Spark-Architecture" title="Spark Architecture">Spark Architecture</a></li>
<li class=""><a href="#How-Spark-works" title="How Spark works?">How Spark works?</a></li>
<li class=""><a href="#Spark-Features" title="Spark Features">Spark Features</a></li>
<li class=""><a href="#Supported-Cluster-Managers" title="Supported Cluster Managers">Supported Cluster Managers</a></li>
</ul>
</li>
<li class=""><a href="#PySpark" title="PySpark">PySpark</a><ul class="nav">
<li class=""><a href="#PySpark-Modules-amp-Packages" title="PySpark Modules &amp; Packages">PySpark Modules &amp; Packages</a></li>
<li class=""><a href="#Install-PySpark" title="Install PySpark">Install PySpark</a><ul class="nav">
<li><a href="#On-Colab" title="On Colab">On Colab</a></li>
<li><a href="#Using-pip" title="Using pip">Using pip</a></li>
<li class=""><a href="#Using-Docker" title="Using Docker">Using Docker</a></li>
<li class=""><a href="#Deploying-Spark-on-Yarn-Cluster" title="Deploying Spark on Yarn Cluster">Deploying Spark on Yarn Cluster</a></li>
<li><a href="#Optional-Install-PySpark-on-HDP-Sandbox" title="[Optional] Install PySpark on HDP Sandbox">[Optional] Install PySpark on HDP Sandbox</a></li>
</ul>
</li>
<li class=""><a href="#Running-Spark-applications" title="Running Spark applications">Running Spark applications</a><ul class="nav">
<li class=""><a href="#pyspark-shell" title="pyspark shell">pyspark shell</a></li>
<li class=""><a href="#spark-submit" title="spark-submit">spark-submit</a></li>
</ul>
</li>
<li><a href="#Python-Package-Management-in-PySpark-apps" title="Python Package Management in PySpark apps">Python Package Management in PySpark apps</a></li>
</ul>
</li>
<li class=""><a href="#Spark-Web-UI" title="Spark Web UI">Spark Web UI</a><ul class="nav">
<li class=""><a href="#Jobs-Tab" title="Jobs Tab">Jobs Tab</a><ul class="nav">
<li class=""><a href="#Job-details" title="Job details">Job details</a></li>
</ul>
</li>
<li class=""><a href="#Stages-Tab" title="Stages Tab">Stages Tab</a><ul class="nav">
<li class=""><a href="#Stage-details" title="Stage details">Stage details</a></li>
</ul>
</li>
</ul>
</li>
<li class=""><a href="#Spark-Core" title="Spark Core">Spark Core</a><ul class="nav">
<li class=""><a href="#Spark-RDD-Operations" title="Spark RDD Operations">Spark RDD Operations</a></li>
<li class=""><a href="#Spark-RDD-Transformations" title="Spark RDD Transformations">Spark RDD Transformations</a></li>
<li class=""><a href="#Spark-RDD-Actions" title="Spark RDD Actions">Spark RDD Actions</a></li>
</ul>
</li>
<li class=""><a href="#Spark-Context" title="Spark Context">Spark Context</a><ul class="nav">
<li class=""><a href="#Import-required-packages" title="Import required packages">Import required packages</a></li>
<li class=""><a href="#Create-a-SparkContext" title="Create a SparkContext">Create a SparkContext</a></li>
</ul>
</li>
<li class=""><a href="#Spark-RDD" title="Spark RDD">Spark RDD</a><ul class="nav">
<li class="invisable-node"><ul class="nav">
<li class=""><a href="#1-using-parallelize-function" title="1. using parallelize() function">1. using parallelize() function</a></li>
<li class=""><a href="#2-using-textFile-or-wholeTextFiles-functions" title="2. using textFile or wholeTextFiles functions">2. using textFile or wholeTextFiles functions</a></li>
</ul>
</li>
<li class=""><a href="#Create-empty-RDD" title="Create empty RDD">Create empty RDD</a></li>
<li class=""><a href="#Repartition-and-Coalesce" title="Repartition and Coalesce">Repartition and Coalesce</a></li>
<li class=""><a href="#PySpark-RDD-Transformations" title="PySpark RDD Transformations">PySpark RDD Transformations</a><ul class="nav">
<li class=""><a href="#map-and-flatMap" title="map and flatMap">map and flatMap</a></li>
<li class=""><a href="#filter" title="filter">filter</a></li>
<li class=""><a href="#distinct" title="distinct">distinct</a></li>
<li class=""><a href="#sample" title="sample">sample</a></li>
<li class=""><a href="#randomSplit" title="randomSplit">randomSplit</a></li>
<li class=""><a href="#mapPartitions-and-mapPartitionsWithIndex" title="mapPartitions and mapPartitionsWithIndex">mapPartitions and mapPartitionsWithIndex</a></li>
<li class=""><a href="#sortBy-and-groupBy" title="sortBy and groupBy">sortBy and groupBy</a></li>
<li class=""><a href="#sortByKey-and-reduceByKey" title="sortByKey and reduceByKey">sortByKey and reduceByKey</a></li>
</ul>
</li>
<li class=""><a href="#PySpark-RDD-Actions" title="PySpark RDD Actions">PySpark RDD Actions</a><ul class="nav">
<li class=""><a href="#collect" title="collect">collect</a></li>
<li class=""><a href="#max-min-first-top-take" title="max, min, first, top, take">max, min, first, top, take</a></li>
<li class=""><a href="#count-countByValue" title="count, countByValue">count, countByValue</a></li>
<li class=""><a href="#reduce-treeReduce" title="reduce, treeReduce">reduce, treeReduce</a></li>
<li class=""><a href="#saveAsTextFile" title="saveAsTextFile">saveAsTextFile</a></li>
</ul>
</li>
<li class=""><a href="#RDD-Persistence" title="RDD Persistence">RDD Persistence</a><ul class="nav">
<li class=""><a href="#RDD-Cache" title="RDD Cache">RDD Cache</a></li>
<li class=""><a href="#RDD-Persist" title="RDD Persist">RDD Persist</a></li>
<li class=""><a href="#RDD-Unpersist" title="RDD Unpersist">RDD Unpersist</a></li>
</ul>
</li>
<li class=""><a href="#Shuffling-in-Spark-engine" title="Shuffling in Spark engine">Shuffling in Spark engine</a></li>
<li class=""><a href="#Shared-Variables" title="Shared Variables">Shared Variables</a><ul class="nav">
<li><a href="#Broadcast-variables" title="Broadcast variables">Broadcast variables</a></li>
<li><a href="#Accumulator-variables" title="Accumulator variables">Accumulator variables</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#References" title="References">References</a></li>
</ul>
</div><div class="toc-menu" style="">
    <a class="expand-toggle expand-all" href="#">Expand all</a>
    <a class="expand-toggle collapse-all" href="#" style="display: none;">Collapse all</a>
    <a class="back-to-top" href="#">Back to top</a>
    <a class="go-to-bottom" href="#">Go to bottom</a>
</div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;"  >
        <div class="toc"><ul class="nav">
<li class=""><a href="#Lab-6---Apache-Spark-RDD" title="Lab 6 - Apache Spark RDD">Lab 6 - Apache Spark RDD</a><ul class="nav">
<li class=""><a href="#Datasets" title="Datasets">Datasets</a></li>
<li class=""><a href="#PySpark-on-Colab" title="PySpark on Colab">PySpark on Colab</a></li>
<li class=""><a href="#Readings" title="Readings">Readings</a></li>
</ul>
</li>
<li class=""><a href="#Agenda" title="Agenda">Agenda</a></li>
<li class=""><a href="#Prerequisites" title="Prerequisites">Prerequisites</a></li>
<li class=""><a href="#Objectives" title="Objectives">Objectives</a></li>
<li class=""><a href="#Intro-to-Apache-Spark-review" title="Intro to Apache Spark [review]">Intro to Apache Spark [review]</a><ul class="nav">
<li class=""><a href="#Core-Concepts-in-Spark" title="Core Concepts in Spark">Core Concepts in Spark</a></li>
<li class=""><a href="#Spark-Application-Model" title="Spark Application Model">Spark Application Model</a></li>
<li class=""><a href="#Spark-Execution-Model" title="Spark Execution Model">Spark Execution Model</a></li>
<li class=""><a href="#Spark-Components" title="Spark Components">Spark Components</a></li>
<li class=""><a href="#Spark-Architecture" title="Spark Architecture">Spark Architecture</a></li>
<li class=""><a href="#How-Spark-works" title="How Spark works?">How Spark works?</a></li>
<li class=""><a href="#Spark-Features" title="Spark Features">Spark Features</a></li>
<li class=""><a href="#Supported-Cluster-Managers" title="Supported Cluster Managers">Supported Cluster Managers</a></li>
</ul>
</li>
<li class=""><a href="#PySpark" title="PySpark">PySpark</a><ul class="nav">
<li class=""><a href="#PySpark-Modules-amp-Packages" title="PySpark Modules &amp; Packages">PySpark Modules &amp; Packages</a></li>
<li class=""><a href="#Install-PySpark" title="Install PySpark">Install PySpark</a><ul class="nav">
<li><a href="#On-Colab" title="On Colab">On Colab</a></li>
<li><a href="#Using-pip" title="Using pip">Using pip</a></li>
<li class=""><a href="#Using-Docker" title="Using Docker">Using Docker</a></li>
<li class=""><a href="#Deploying-Spark-on-Yarn-Cluster" title="Deploying Spark on Yarn Cluster">Deploying Spark on Yarn Cluster</a></li>
<li><a href="#Optional-Install-PySpark-on-HDP-Sandbox" title="[Optional] Install PySpark on HDP Sandbox">[Optional] Install PySpark on HDP Sandbox</a></li>
</ul>
</li>
<li class=""><a href="#Running-Spark-applications" title="Running Spark applications">Running Spark applications</a><ul class="nav">
<li class=""><a href="#pyspark-shell" title="pyspark shell">pyspark shell</a></li>
<li class=""><a href="#spark-submit" title="spark-submit">spark-submit</a></li>
</ul>
</li>
<li><a href="#Python-Package-Management-in-PySpark-apps" title="Python Package Management in PySpark apps">Python Package Management in PySpark apps</a></li>
</ul>
</li>
<li class=""><a href="#Spark-Web-UI" title="Spark Web UI">Spark Web UI</a><ul class="nav">
<li class=""><a href="#Jobs-Tab" title="Jobs Tab">Jobs Tab</a><ul class="nav">
<li class=""><a href="#Job-details" title="Job details">Job details</a></li>
</ul>
</li>
<li class=""><a href="#Stages-Tab" title="Stages Tab">Stages Tab</a><ul class="nav">
<li class=""><a href="#Stage-details" title="Stage details">Stage details</a></li>
</ul>
</li>
</ul>
</li>
<li class=""><a href="#Spark-Core" title="Spark Core">Spark Core</a><ul class="nav">
<li class=""><a href="#Spark-RDD-Operations" title="Spark RDD Operations">Spark RDD Operations</a></li>
<li class=""><a href="#Spark-RDD-Transformations" title="Spark RDD Transformations">Spark RDD Transformations</a></li>
<li class=""><a href="#Spark-RDD-Actions" title="Spark RDD Actions">Spark RDD Actions</a></li>
</ul>
</li>
<li class=""><a href="#Spark-Context" title="Spark Context">Spark Context</a><ul class="nav">
<li class=""><a href="#Import-required-packages" title="Import required packages">Import required packages</a></li>
<li class=""><a href="#Create-a-SparkContext" title="Create a SparkContext">Create a SparkContext</a></li>
</ul>
</li>
<li class=""><a href="#Spark-RDD" title="Spark RDD">Spark RDD</a><ul class="nav">
<li class="invisable-node"><ul class="nav">
<li class=""><a href="#1-using-parallelize-function" title="1. using parallelize() function">1. using parallelize() function</a></li>
<li class=""><a href="#2-using-textFile-or-wholeTextFiles-functions" title="2. using textFile or wholeTextFiles functions">2. using textFile or wholeTextFiles functions</a></li>
</ul>
</li>
<li class=""><a href="#Create-empty-RDD" title="Create empty RDD">Create empty RDD</a></li>
<li class=""><a href="#Repartition-and-Coalesce" title="Repartition and Coalesce">Repartition and Coalesce</a></li>
<li class=""><a href="#PySpark-RDD-Transformations" title="PySpark RDD Transformations">PySpark RDD Transformations</a><ul class="nav">
<li class=""><a href="#map-and-flatMap" title="map and flatMap">map and flatMap</a></li>
<li class=""><a href="#filter" title="filter">filter</a></li>
<li class=""><a href="#distinct" title="distinct">distinct</a></li>
<li class=""><a href="#sample" title="sample">sample</a></li>
<li class=""><a href="#randomSplit" title="randomSplit">randomSplit</a></li>
<li class=""><a href="#mapPartitions-and-mapPartitionsWithIndex" title="mapPartitions and mapPartitionsWithIndex">mapPartitions and mapPartitionsWithIndex</a></li>
<li class=""><a href="#sortBy-and-groupBy" title="sortBy and groupBy">sortBy and groupBy</a></li>
<li class=""><a href="#sortByKey-and-reduceByKey" title="sortByKey and reduceByKey">sortByKey and reduceByKey</a></li>
</ul>
</li>
<li class=""><a href="#PySpark-RDD-Actions" title="PySpark RDD Actions">PySpark RDD Actions</a><ul class="nav">
<li class=""><a href="#collect" title="collect">collect</a></li>
<li class=""><a href="#max-min-first-top-take" title="max, min, first, top, take">max, min, first, top, take</a></li>
<li class=""><a href="#count-countByValue" title="count, countByValue">count, countByValue</a></li>
<li class=""><a href="#reduce-treeReduce" title="reduce, treeReduce">reduce, treeReduce</a></li>
<li class=""><a href="#saveAsTextFile" title="saveAsTextFile">saveAsTextFile</a></li>
</ul>
</li>
<li class=""><a href="#RDD-Persistence" title="RDD Persistence">RDD Persistence</a><ul class="nav">
<li class=""><a href="#RDD-Cache" title="RDD Cache">RDD Cache</a></li>
<li class=""><a href="#RDD-Persist" title="RDD Persist">RDD Persist</a></li>
<li class=""><a href="#RDD-Unpersist" title="RDD Unpersist">RDD Unpersist</a></li>
</ul>
</li>
<li class=""><a href="#Shuffling-in-Spark-engine" title="Shuffling in Spark engine">Shuffling in Spark engine</a></li>
<li class=""><a href="#Shared-Variables" title="Shared Variables">Shared Variables</a><ul class="nav">
<li><a href="#Broadcast-variables" title="Broadcast variables">Broadcast variables</a></li>
<li><a href="#Accumulator-variables" title="Accumulator variables">Accumulator variables</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#References" title="References">References</a></li>
</ul>
</div><div class="toc-menu" style="">
    <a class="expand-toggle expand-all" href="#">Expand all</a>
    <a class="expand-toggle collapse-all" href="#" style="display: none;">Collapse all</a>
    <a class="back-to-top" href="#">Back to top</a>
    <a class="go-to-bottom" href="#">Go to bottom</a>
</div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.0/js/bootstrap.min.js" integrity="sha256-kJrlY+s09+QoWjpkOrXXwhxeaoDz9FW5SaxF8I0DibQ=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
